<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CLIP – Documentation</title>
    <link>https://clip-hpc.github.io/docs/</link>
    <description>Recent content in Documentation on CLIP</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	  <atom:link href="https://clip-hpc.github.io/docs/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Application and Software on CBE</title>
      <link>https://clip-hpc.github.io/docs/cbe/software/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/cbe/software/</guid>
      <description>
        
        
        &lt;h2 id=&#34;environment-modules&#34;&gt;Environment Modules&lt;/h2&gt;
&lt;p&gt;The Modules package provides for dynamic modification of a user&amp;rsquo;s environment with module files. Each module file contains the information needed to configure the shell for an application. Once the package is initialized, the environment can be modified dynamically on a per-module basis using the module command that interprets module files. Typically, module files instruct the module command to alter or set shell environment variables, such as PATH or MANPATH. Module files may be shared by many users on a system and users may have their own collection to supplement or replace the shared module files.&lt;/p&gt;
&lt;p&gt;The Modules environment management package offers support for dynamic modification of the user environment via modules. Each module contains all the information needed to configure the shell for a particular application. To make major changes in your user environment, such as switching to a different compiler or a different version of a library, use the appropriate Modules commands to select the desired modules.&lt;br&gt;
The advantage in using Modules is that users are not required to specify explicit paths (&lt;code&gt;$PATH&lt;/code&gt; or &lt;code&gt;$LD_LIBRARY_PATH&lt;/code&gt;) for different executable versions or to set the &lt;code&gt;$MANPATH&lt;/code&gt; and other environment variables manually. Instead, all the information required in order to use a given piece of software is embedded in the module and set automatically when you load the module.&lt;/p&gt;
&lt;p&gt;In general, the simplest way to make certain that the elements of an applications environment function correctly together is by using the modules software and trusting it to keep track of paths and environment variables, and avoiding embedding specific directory paths into startup and job scripts.&lt;/p&gt;
&lt;p&gt;For CBE the environment modules package available across the compute infrastructure is called Lmod and provides an implementation of the environment modules  in the scripting language Lua. It understands classical module files as written in the Tcl language and module files written in Lua and has many benefits over the classic environment modules whoever all files are written in Lua.&lt;/p&gt;
&lt;h3 id=&#34;environment-modules-package-commands&#34;&gt;Environment Modules Package Commands&lt;/h3&gt;
&lt;p&gt;The following table gives an overview of available module-commands:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ml load &lt;code&gt;module-name&lt;/code&gt;/&lt;code&gt;version&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Loads a given module file and modifies shell accordingly.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml help&lt;/td&gt;
&lt;td&gt;Displays a list of all available module commands&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml unload &lt;code&gt;module-name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Unload a given module and removes shell modification&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml whatis &lt;code&gt;module-name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Displays a short description of the software and home page of the package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml display &lt;code&gt;module-name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Show all variables set by the module and see what is going to be changed in the current shell session&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml&lt;/td&gt;
&lt;td&gt;Shows a list of loaded modules in the current session&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml av&lt;/td&gt;
&lt;td&gt;Displays a list of all available modules in the system&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml swap &lt;code&gt;module-name&lt;/code&gt; &lt;code&gt;module-name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Swaps the module with another module (unload/load)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml purge&lt;/td&gt;
&lt;td&gt;Remove all loaded modules from the current session&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml use &lt;code&gt;path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Add a given path and as an additional path to look for module files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml unuse &lt;code&gt;path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Remove a path from the search path for modules&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;software-trees&#34;&gt;Software Tree(s)&lt;/h2&gt;
&lt;p&gt;IT service install highly optimized software in  a CPU specific way. All optimizations options for the CPUs of the compute nodes are selected.&lt;/p&gt;
&lt;p&gt;The software trees are published under the &lt;code&gt;/software&lt;/code&gt; directory and the system&amp;rsquo;s default configuration picks up one and only one tree. Mixing modules across different trees is not supported and should be left to expert users.&lt;/p&gt;
&lt;h2 id=&#34;lifecycle-policy-of-installed-software&#34;&gt;Lifecycle Policy of Installed Software&lt;/h2&gt;
&lt;p&gt;The initial system contains one software tree installed under path &lt;code&gt;/software/&lt;/code&gt;. Application software will never be reinstalled with different features and versions are never removed. Once a new software tree is and set as the system&amp;rsquo;s default software tree  users will be notified. Older software trees will stay in place and will not be removed during the life time of the system.&lt;/p&gt;
&lt;h2 id=&#34;module-names&#34;&gt;Module Names&lt;/h2&gt;
&lt;p&gt;The module contain not only the name of the application but also specify which compiler (toolchain) was used to compile them. If no compiler toolchain is specified then the Linux distributions base installation compiler was used for the compilation. This is usually done on software that is either used to build other software or where optimizations is of no concern.&lt;/p&gt;


&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Lowercase&lt;/h4&gt;
Please note that the module naming scheme is all lower case for all installed software.
&lt;/div&gt;

&lt;h2 id=&#34;compiler-toolchains&#34;&gt;Compiler Toolchains&lt;/h2&gt;
&lt;p&gt;Are basically a (set of) compilers together with a set of libraries that provide additional support functionality that is commonly required to build software to satisfy build-time dependencies. In the world of High Performance Computing this usually consists of an library for MPI (inter-process communication over a network), BLAS/LAPACK (linear algebra routines) and FFT (Fast Fourier Transforms) as the most basic &amp;ldquo;toolset&amp;rdquo;. If a piece of software does not benefit from or is not implemented on top of a BLAS library or is not an MPI program then this functionality does not affect the program in any sense.&lt;/p&gt;
&lt;p&gt;In general toolchains cannot be mixed, usually not even in different version and require a recompilation of all the software that needs to be compiled with a newer version of the toolchain. This is mostly true for software written in C++ and Fortran, all MPI packages and is generally irrelevant for programs in C, Python, etc.&lt;/p&gt;
&lt;p&gt;Programming languages written on top of C++ or Fortran like R that have binary dependencies that need compilation are dependant on the compiler and version for the non R parts. The same is true for Python software using binary libraries implemented in the mention run times.&lt;/p&gt;
&lt;p&gt;IT services will generally provide relatively recent version of the GNU programming environment and the commercial Intel programming environment. It is essential that software and all of it&amp;rsquo;s dependencies are available for the application that don&amp;rsquo;t depend on their underlying operating system environment to have the dependencies and all the needed version available.&lt;/p&gt;
&lt;p&gt;In general all tools are either compiled with the FOSS toolchain or the INTEL toolchain and the toolchain is instantiated once a module is loaded that was compiled via one or the other toolchain as a dependency.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Toolchain&lt;/th&gt;
&lt;th&gt;Components&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Use Cases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;foss/2018b&lt;/td&gt;
&lt;td&gt;GCC (c/c++, fortran,&amp;hellip;), OpenMPI, OpenBLAS, ScLAPACK, FFTW&lt;/td&gt;
&lt;td&gt;Open source toolchain in the 2018b version based on GCC 8.0 and OpenMPI 3.0.2&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;Toolchain for general purpose use&lt;/li&gt;&lt;li&gt;High performance&lt;/li&gt;&lt;li&gt;Based on free and open source software (FOSS)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Known Caveats</title>
      <link>https://clip-hpc.github.io/docs/knowncaveats/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/knowncaveats/</guid>
      <description>
        
        
        &lt;h2 id=&#34;batch-jobs&#34;&gt;Batch Jobs&lt;/h2&gt;
&lt;h3 id=&#34;graphical-user-interfaces-in-interactive-batch-sessions&#34;&gt;Graphical User Interfaces in Interactive Batch Sessions&lt;/h3&gt;
&lt;p&gt;When you enable X11 forwarding on your SSH connection to the cluster, you will be able to work with graphical user interfaces on the login nodes. When you work in an interactive batch session to keep load off the login nodes (which you should), the following approach is required:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ssh -Y username@cbe.vbc.ac.at
salloc bash -c &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#39;ssh -Y $(scontrol show hostnames | head -n 1)&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;### module load and run your application program, here it&amp;#39;s R&lt;/span&gt;
module load r/3.5.1-foss-2018b 

R
x11&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;rstudio&#34;&gt;RStudio&lt;/h2&gt;
&lt;h3 id=&#34;package-installation-fails&#34;&gt;Package installation fails&lt;/h3&gt;
&lt;p&gt;Rstudio (via XPRA web interface) uses one of the normal R modules on CBE. In case the installation of custom R packages fails, try to install from the command line:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ssh username@cbe.vbc.ac.at
module load r/3.5.1-foss-2018b		&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;### ... or whichever version Rstudio is using at the time&lt;/span&gt;
R

&amp;gt; install.packages&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;PackageName&amp;#34;&lt;/span&gt;,dependencies &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt; TRUE&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;ssh-connection-issues-to-cbe&#34;&gt;SSH connection issues to CBE&lt;/h2&gt;
&lt;h3 id=&#34;windows-10-and-wsl-and-vpn&#34;&gt;Windows 10 and WSL and VPN&lt;/h3&gt;
&lt;p&gt;If you use Windows 10 and a WSL (Windows Subsystem for Linux) terminal such as bash to connect via ssh to cbe while being connected with VPN you might see following error:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ssh cbe.vbc.ac.at -l username
ssh: Could not resolve hostname cbe.vbc.ac.at: Name or service not known
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is due to a &lt;a href=&#34;https://github.com/Microsoft/WSL/issues/1350&#34;&gt;bug&lt;/a&gt; in WSL which breaks the DNS name resolution of &lt;code&gt;cbe.vbc.ac.at&lt;/code&gt;. There are currently 3 workarounds ranging from simple to advanced.&lt;/p&gt;
&lt;h4 id=&#34;solution-1-simple&#34;&gt;Solution 1 (simple):&lt;/h4&gt;
&lt;p&gt;Don&amp;rsquo;t use WSL but use the Windows 10 native terminal (cmd.exe) or the &lt;a href=&#34;https://www.putty.org/&#34;&gt;Putty ssh client&lt;/a&gt; to connect to cbe.&lt;/p&gt;
&lt;h4 id=&#34;solution-2-medium&#34;&gt;Solution 2 (medium):&lt;/h4&gt;
&lt;p&gt;Connect directly the IP of one of the CBE login nodes instead of the dns name.&lt;br&gt;
First determine the IP of one of the CBE login nodes using the Windows 10 native terminal:&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 823px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/knowncaveats/cmd_huc87fc707be43f1e736186564d3a5432e_135389_813x0_resize_catmullrom_2.png&#34; width=&#34;813&#34; height=&#34;211&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Find out IP address
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;Then connect using this IP:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ ssh 172.24.96.13 -l username
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In order to avoid remembering the IP address everytime you connect to CBE, you can add CBE to your ssh config. In the WSL terminal edit the file &lt;code&gt;~/.ssh/config&lt;/code&gt; (in case it doesn&amp;rsquo;t exist just create it) using &lt;code&gt;vim&lt;/code&gt; or any other editor and add following section:&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;

Replace &lt;code&gt;username&lt;/code&gt; with your actual username
&lt;/div&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;Host cbe
    User username
    HostName 172.24.96.13
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can then connect to cbe.vbc.ac.at simply with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ ssh cbe
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;solution-3-advanced&#34;&gt;Solution 3 (advanced):&lt;/h4&gt;
&lt;p&gt;Add the campus DNS server to /etc/resolve.conf&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Warning&lt;/h4&gt;
You can potentially break DNS name resolution inside WSL. So be careful
&lt;/div&gt;

&lt;p&gt;Edit the &lt;code&gt;/etc/resolve.conf&lt;/code&gt; file in your WSL terminal and add following line to the end of the section and save the file:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;nameserver 172.16.80.12
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Important&lt;/h4&gt;
Beware that this nameserver is only reachable and thus working when connected via VPN
&lt;/div&gt;

&lt;p&gt;You can then connect to cbe using:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;$ ssh cbe.vbc.ac.at -l username
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
      </description>
    </item>
    
    <item>
      <title>Docs: Storage on CBE</title>
      <link>https://clip-hpc.github.io/docs/cbe/storage/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/cbe/storage/</guid>
      <description>
        
        
        &lt;p&gt;The CBE cluster has several IMP/IMBA/GMI storage devices and clusters attached to it and mounted within the file system. This consists of devices for storing scientific data, the software trees and user home directories specific to CBE and a global parallel shared file system between the CBE nodes.&lt;/p&gt;
&lt;p&gt;The Isilon clusters (NFS) are globally available for storing group data and an site wide scratch file system available on all clusters, workstations and other devices connected to the IMP/IMBA/GMI network.&lt;/p&gt;
&lt;p&gt;The GMI NetApp file servers are globally available for storing group data and an site wide scratch file system available on all clusters, workstations and other devices connected to the IMP/IMBA/GMI network.&lt;/p&gt;
&lt;p&gt;In addition to this a very high performance file system  (BeegFS) is implemented within CBE&amp;rsquo;s flash/SSD compute nodes that offers and aggregated storage from all the individual SSDs within those nodes to all other compute nodes.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NetApp Science Storage&lt;br&gt;(/scratch and /resources)&lt;/th&gt;
&lt;th&gt;NetApp Infra Storage&lt;br&gt;(/users)&lt;/th&gt;
&lt;th&gt;CBE Fast Scratch&lt;br&gt;(/scratch-cbe)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;clustertmp mounted on /scratch and on the compute nodes and all user facing nodes&lt;/li&gt;&lt;li&gt;All compute clusters (except MENDEL) have this storage mounted&lt;/li&gt;&lt;li&gt;groups mounted on /groups on all&lt;/li&gt;&lt;li&gt;compute nodes and all user facing nodes&lt;/li&gt;&lt;li&gt;resources mounted on /resources on all compute nodes and all user facing nodes&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;CBE specific home directories on /users&lt;/li&gt;&lt;li&gt;GMI specific projects are mounted under /groups/&amp;lt;group-name&amp;gt;/projects/project-name&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;/scratch-cbe on all compute nodes from local SSD flash drives on the SSD flash nodes&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;storage-location-use-cases&#34;&gt;Storage Location Use Cases&lt;/h2&gt;
&lt;p&gt;The following table gives an overview of available module-commands:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Storage Location&lt;/th&gt;
&lt;th&gt;Use case&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;/scratch-cbe&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;storage of data and files required by jobs and during job execution&lt;/li&gt;&lt;li&gt;highest level of performance independent of IO profile&lt;/li&gt;&lt;li&gt;automatically purged after 30 days&lt;/li&gt;&lt;li&gt;copy data from /scratch  or /groups to /scratch-cbe and back for high performance access&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/scratch&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;storage of all bulk scientific temporary data that is generated while jobs are running that is of general purpose nature&lt;/li&gt;&lt;li&gt;storage of all files that require high performance access (preferably sequential IO)&lt;/li&gt;&lt;li&gt;automatically purged after 30 days &lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/groups/&amp;lt;group-name&amp;gt;/projects&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;GMI &amp;ldquo;projects folder&amp;rdquo; mounted into the groups hierarchy&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/groups and /resources&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;storage all long term data&lt;/li&gt;&lt;li&gt;avoid processing data on this location due to performance penalties (1:8 ratio against clustertmp) for older files&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/users&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;storage of configuration files and program files&lt;/li&gt;&lt;li&gt;do not keep data here&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/tmp on compute nodes&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;Redirected to /scratch-cbe/tmp-inst/&amp;lt;user-name&amp;gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;staging-data&#34;&gt;Staging Data&lt;/h2&gt;
&lt;p&gt;The fast parallel BeeGFS file system hosting (mainly) &lt;code&gt;/scratch-cbe&lt;/code&gt;, as the mount point implies, is intended purely as a scratch space for fast access during compute jobs. Users are supposed to copy the respective data to &lt;code&gt;/scratch-cbe&lt;/code&gt; shortly before computing and move results out of &lt;code&gt;/scratch-cbe&lt;/code&gt; afterwards. This is called staging. The 30 days lifetime are merely a grace period in this usage scenario.&lt;/p&gt;
&lt;p&gt;To assist in implementing of a suitable staging practice with your compoute jobs on CLIP/CBE, a ready-made staging script is provided.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;user.name@clip-login-0 ~&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;$ stage --help
usage: /software/system/utils/stage source_folder destination_folder
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This script will spawn single copy jobs on 20 compute nodes in parallel in order to maximize throughput. Of course, you can always use this script to copy your data in and out of &lt;code&gt;/scratch-cbe&lt;/code&gt; manually. Also, when used with a single batch job or a job array, you can easily insert staging commands at the beginning and end of your batch script.&lt;/p&gt;
&lt;p&gt;When you intend to use the staged data in multiple independent compute jobs, it is advisable to create separate staging jobs and link them before and after the actual compute jobs. This can be done via the &lt;code&gt;--dependency&lt;/code&gt; parameter to the &lt;code&gt;sbatch&lt;/code&gt; command. Please see the sbatch man page for details or &lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;click here&lt;/a&gt; for a web version.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Web Access (XPRA)</title>
      <link>https://clip-hpc.github.io/docs/xpra/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/xpra/</guid>
      <description>
        
        
        &lt;p&gt;The CBE compute cluster can also be accessed from a web-based graphical interface called XPRA that is available on the &lt;a href=&#34;https://it.vbc.ac.at/clip/cbe/xpra&#34;&gt;IT Website&lt;/a&gt;. The idea of a web based cluster access is to run compute intense jobs that require a graphical user interface (GUI) – like Rstudio and Fiji – directly on a cluster node and interact with the application through your web browser.&lt;/p&gt;
&lt;p&gt;The XPRA section of our self service portal offers you some common GUI Applications. Please specify the resources for your job according to your needs. After you created the job it takes some time (usually a few seconds) to submit it to the cluster. As soon as the cluster accepts your request, you will see the job in the state queued in your job-list. It depends on the cluster utilization how long it takes to actually start your job on a compute node. Jobs with small resource demands are prioritized (see CBE documentation). When your job is running on a cluster node, you will see a link next to your job entry. It will open a new browser window that connects you directly to your application.&lt;/p&gt;
&lt;p&gt;Keep in mind that your job is executed as your USER. If you have no CBE cluster account yet, your XPRA session will fail so, please apply for cluster access &lt;a href=&#34;https://jira.vbc.ac.at/servicedesk/customer/portal/1/create/13&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We have integrated all common file storages in the graphical job runner – group and user directories as well as the temporary scratch folders.&lt;/p&gt;
&lt;p&gt;If you are missing any GUI tool that you want to run on the cluster, please issue a HPC Application Request ticket &lt;a href=&#34;https://jira.vbc.ac.at/servicedesk/customer/portal/1/create/12&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Advanced SLURM Guide</title>
      <link>https://clip-hpc.github.io/docs/cbe/slurm/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/cbe/slurm/</guid>
      <description>
        
        
        &lt;h2 id=&#34;slurm&#34;&gt;SLURM&lt;/h2&gt;
&lt;p&gt;As a cluster resource manager (CPUs, memory, custom resources like SSDs, etc.), SLURM has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a job) on the set of allocated node or for parallel jobs on nodes. Finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/p&gt;
&lt;h3 id=&#34;slurm-terminology&#34;&gt;SLURM Terminology&lt;/h3&gt;
&lt;p&gt;The entities managed by these SLURM include nodes, the compute resource in SLURM, &lt;em&gt;partitions&lt;/em&gt;, which group nodes into logical sets, &lt;em&gt;jobs&lt;/em&gt;, or allocations of resources assigned to a user for a specified amount of time, and &lt;em&gt;job steps&lt;/em&gt;, which are sets of (possibly parallel) tasks within a job. Each job in the priority-ordered queue is allocated nodes within a single partition. Nodes can be members of more than node partition.&lt;/p&gt;
&lt;p&gt;Once a job is assigned one node or a set of nodes, the user is able to initiate work in the form of job steps in any configuration within the allocation. For instance, a single job step may be started that utilizes all nodes allocated to the job if supported by the software, or several job steps may independently use a portion of the allocation linke single or multiple cores on one compute node.&lt;/p&gt;
&lt;h3 id=&#34;slurm-command-line-utilities&#34;&gt;SLURM Command Line Utilities&lt;/h3&gt;
&lt;p&gt;Users interact with SLURM through command line utilities. In this section we just introduce the most important commands available to users while the use of these will be covered in a later section of this document.&lt;/p&gt;
&lt;p&gt;Please note that on CBE the BASH shell is configured to tab-complete the options available to these SLURM command line utilities.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Use Case&lt;/th&gt;
&lt;th&gt;Notes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;srun&lt;/td&gt;
&lt;td&gt;submitting jobs for execution and optionally controlling it interactively&lt;/td&gt;
&lt;td&gt;&lt;code&gt;srun&lt;/code&gt; is the user interface to accessing resources managed by SLURM. Users may utilize &lt;code&gt;srun&lt;/code&gt; to allocate resources, submit batch jobs, run jobs interactively, attach to currently running jobs, or launch a set of parallel tasks (job step) for a running job.&lt;br&gt;&lt;code&gt;srun&lt;/code&gt; can either create an on demand resource selection before starting a possibly interactive command or launch tasks into an resource allocation crated by running salloc&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scancel&lt;/td&gt;
&lt;td&gt;terminating a pending or running job.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;scancel&lt;/code&gt;terminates queued jobs or signals running jobs or job steps. The default signal is &lt;code&gt;SIGKILL&lt;/code&gt;, which indicates a request to terminate the specified job or job step. &lt;code&gt;scancel&lt;/code&gt; identifies the job(s) to be signaled through user specification of the SLURM job id, job step id, user name, partition name, and/or job state. If a job id is supplied, all job steps associated with the job are affected as well as the job and its resource allocation. If a job step id is supplied, only that job step is affected. scancel can only be executed by the job’s owner or a privileged user.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;squeue&lt;/td&gt;
&lt;td&gt;monitoring job queues&lt;/td&gt;
&lt;td&gt;&lt;code&gt;squeue&lt;/code&gt; reports the state of SLURM jobs. It can filter these jobs input specification of job state (RUN, PENDING, etc.), job id, user name, job name, etc. If no specification is supplied, the state of all pending and running jobs is reported. squeue also has a variety of sorting and output options&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sinfo&lt;/td&gt;
&lt;td&gt;show a summary of partition and node information&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sbatch&lt;/td&gt;
&lt;td&gt;submit a job script for later execution. The script will typically contain one or more srun commands to launch parallel tasks&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;salloc&lt;/td&gt;
&lt;td&gt;allocate resources and potentially use them for srun&lt;/td&gt;
&lt;td&gt;allocates resources for a job in real time. Typically this is used to allocate resources and spawn a shell. The shell is then used to execute srun commands to launch serial or parallel tasks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scontrol&lt;/td&gt;
&lt;td&gt;retrieve SLURM information on partitions, limits, jobs, etc&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sattach&lt;/td&gt;
&lt;td&gt;attaches the terminal stdin/stdout to the job&amp;rsquo;s stdin/stdout&lt;/td&gt;
&lt;td&gt;sattach allows a terminal to attach/reattach to a job&amp;rsquo;s output to monitor it&amp;rsquo;s output/errors and progression&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sprio&lt;/td&gt;
&lt;td&gt;reports priority of queueing jobs&lt;/td&gt;
&lt;td&gt;list output of individual components that affected priority/order in queue for waiting jobs&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;custom-vbc-slurm-utilities&#34;&gt;Custom VBC SLURM Utilities&lt;/h3&gt;
&lt;p&gt;The following utilities are provided in addition to the SLURM commands as convenient shortcuts for quicker access to relevant information on the queue&amp;rsquo;s status, available and used resources, limits configured etc.&lt;/p&gt;
&lt;h4 id=&#34;slurm-ltcommandgt&#34;&gt;slurm &amp;lt;command&amp;gt;&lt;/h4&gt;
&lt;p&gt;Show status of partitions, jobs, node usage history, priorties etc.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;john.doe@clip-login-0 ~&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;$ slurm 

Show or watch job queue:
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; queue     show own &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; q &amp;lt;user&amp;gt;  show user&lt;span style=&#34;color:#a40000&#34;&gt;&amp;#39;&lt;/span&gt;s &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; quick     show quick overview of own &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; shorter   sort and compact entire queue by job size
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; short     sort and compact entire queue by priority
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; full      show everything
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;w&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;q&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;qq&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;ss&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;s&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;f&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; shorthands &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;for&lt;/span&gt; above!

 slurm qos               show job service classes
 slurm top &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;queue&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;all&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;   show summary of active users

Show detailed information about jobs:
 slurm prio &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;all&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;short&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;  show priority components
 slurm j&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;job &amp;lt;jobid&amp;gt;     show everything &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;else&lt;/span&gt;
 slurm steps &amp;lt;jobid&amp;gt;     show memory usage of running srun job steps

Show usage and fair-share values from accounting database:
 slurm h&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;&lt;span style=&#34;color:#204a87&#34;&gt;history&lt;/span&gt; &amp;lt;time&amp;gt;  show &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt; finished since, e.g. &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;1day&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;default&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
 slurm shares

Show nodes and resources in the cluster:
 slurm p&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;partitions      all partitions
 slurm n&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;nodes           all cluster nodes
 slurm c&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;cpus            total cpu cores in use
 slurm cpus &amp;lt;partition&amp;gt;  cores available to partition, allocated and free
 slurm cpus &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;         cores/memory reserved by running &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm cpus queue        cores/memory required by pending &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm features          List features and GRES
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;jobinfo-ltjobidgt&#34;&gt;jobinfo &amp;lt;jobid&amp;gt;&lt;/h4&gt;
&lt;p&gt;Shows status and resource consumption of jobs&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;john.doe@clip-login-0 ~&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;$ jobinfo &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;22&lt;/span&gt;
Name                : bash
User                : john.doe
Partition           : c
Nodes               : clip-c2-0
Cores               : &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt;
State               : COMPLETED
Submit              : 2019-07-19T11:20:14
Start               : 2019-07-19T11:20:14
End                 : 2019-07-19T11:21:37
Reserved walltime   : 08:00:00
Used walltime       : 00:01:23
Used CPU &lt;span style=&#34;color:#204a87&#34;&gt;time&lt;/span&gt;       : --
% User &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;Computation&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;:  0.00%
% System &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;I/O&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;      : 30.19%
Mem reserved        : 4G/core
Max Mem used        : 2.32M &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;clip-c2-0&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
Max Disk Write      : 10.24K &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;clip-c2-0&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
Max Disk Read       : 1.99M &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;clip-c2-0&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;cbe-slurm&#34;&gt;CBE SLURM&lt;/h2&gt;
&lt;h3 id=&#34;available-resources&#34;&gt;Available Resources&lt;/h3&gt;
&lt;p&gt;The hardware resources available to CLIP instances differ from the resources available to the physical compute nodes running the virtual&lt;/p&gt;
&lt;h3 id=&#34;compute-node-types--hardware-resources&#34;&gt;Compute Node Types &amp;amp; Hardware Resources&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Node&lt;/th&gt;
&lt;th&gt;Count&lt;/th&gt;
&lt;th&gt;Cores&lt;/th&gt;
&lt;th&gt;Memory&lt;/th&gt;
&lt;th&gt;Network&lt;/th&gt;
&lt;th&gt;GPUs&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;clip-c1-x&lt;/td&gt;
&lt;td&gt;39&lt;/td&gt;
&lt;td&gt;2 x 11&lt;/td&gt;
&lt;td&gt;85 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-c2-x&lt;/td&gt;
&lt;td&gt;120&lt;/td&gt;
&lt;td&gt;2 x 19&lt;/td&gt;
&lt;td&gt;170 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-m1-x&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2 x 11&lt;/td&gt;
&lt;td&gt;4 TB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-m2-x&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;8 x 76&lt;/td&gt;
&lt;td&gt;85 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-g1-x&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2 x 7&lt;/td&gt;
&lt;td&gt;170 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;8x NVIDIA P100 (12GB VRAM)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-g2-x&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2 x 15&lt;/td&gt;
&lt;td&gt;170 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;4x NVIDIA V100 (32GB VRAM)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;node-preference&#34;&gt;Node Preference&lt;/h3&gt;
&lt;p&gt;Given that more than one node type could serve the limits as requested through jobs the nodes with the lower weight are more likely to be selected to execute the job. This implies that nodes requesting a lower amount of CPUs and memory will be preferably execute on nodes without GPUs, exrtra large memory, SSD drives, etc&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;clip-c1-x&lt;/th&gt;
&lt;th&gt;clip-c2-x&lt;/th&gt;
&lt;th&gt;clip-m1-x&lt;/th&gt;
&lt;th&gt;clip-m2-x&lt;/th&gt;
&lt;th&gt;clip-g1-x&lt;/th&gt;
&lt;th&gt;clip-g2-x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Weight&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;partitions&#34;&gt;Partitions&lt;/h3&gt;
&lt;p&gt;On CLIP Batch Environment the nodes in the cluster are grouped in partitions. As already introduced partitions are a logical grouping of nodes of a particular quality or configuration. Within CLIP Batch Environment the following special nodes exist. The SLURM notion of partitions can be understood as queues within other resource managers like PBS, LSF, etc.&lt;/p&gt;
&lt;h4 id=&#34;rationale&#34;&gt;Rationale&lt;/h4&gt;
&lt;p&gt;SLURM partition configuration is done is such a way that jobs that could potentially run on any node type can be executed on any node of a given type. Ie all &amp;lsquo;regular&amp;rsquo; compute nodes are grouped into one partition, all &amp;lsquo;high memory&amp;rsquo; nodes are grouped in another partition and so on. Jobs that should or must be executed on a certain node type will need to explicitly either select the partition like the high memory node partition or the GPU partition.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 823px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/cbe/slurm/cbe-nodes_hube6f268fa83de819f54b31abfd809e15_293759_813x0_resize_catmullrom_2.png&#34; width=&#34;813&#34; height=&#34;567&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
CLIP Nodes
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Partition:&lt;/th&gt;
&lt;th&gt;c&lt;/th&gt;
&lt;th&gt;m&lt;/th&gt;
&lt;th&gt;g&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Use Case&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;default partition&lt;/strong&gt; where jobs can be submitted to&lt;/td&gt;
&lt;td&gt;special use partition for jobs that require from these nodes with are significantly enlarged main memory/RAM within compute nodes&lt;/td&gt;
&lt;td&gt;special use partition for jobs that profit from GPU accelerators within GPU nodes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resources&lt;/td&gt;
&lt;td&gt;Contains all regular high core count (clip-c2-x) and high clock (clip-c1-x) compute nodes&lt;/td&gt;
&lt;td&gt;Contains only nodes of the large  Memory Node type for large memory (clip-m2-x) and very large memory (clip-m1-x) nodes&lt;/td&gt;
&lt;td&gt;Contains only nodes of the GPU Node type with both GPU types (clip-g1-x &amp;amp; clip-g2-x)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;resource-limits-qos&#34;&gt;Resource Limits (QOS)&lt;/h3&gt;
&lt;p&gt;Within SLURM resource limitations are configured through named objects called QOS (Quality of Service). QOS objects specify limits in on resource like CPUs, memory, etc , and maximum execution time (wall time). QOS objects be attached to partitions and users may specify QOS objects on job submission (srun, salloc, sbatch).&lt;/p&gt;
&lt;p&gt;The CBE resource isolation is an configured to use Linux cgroups a feature that setups a resource &amp;ldquo;jail&amp;rdquo; enforced by the Kernel to guarantee resource and reduce job interference between jobs sharing and executing on the same compute node to a minimum.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;QOS:&lt;/th&gt;
&lt;th&gt;short&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;long&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Use Case&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Default&lt;/strong&gt; QOS (ie when no QOS is selected then this QOS is assumed)&lt;br&gt;Short running jobs with large resource limits (fast turnaround)&lt;/td&gt;
&lt;td&gt;&amp;ldquo;Average&amp;rdquo; jobs&lt;/td&gt;
&lt;td&gt;Very long running jobs with small resource limits&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Limits&lt;/td&gt;
&lt;td&gt;Dynamic (check out put of &amp;lsquo;slurm qos&amp;rsquo;)&lt;/td&gt;
&lt;td&gt;Dynamic (check out put of &amp;lsquo;slurm qos&amp;rsquo;)&lt;/td&gt;
&lt;td&gt;Dynamic (check out put of &amp;lsquo;slurm qos&amp;rsquo;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Walltime&lt;/td&gt;
&lt;td&gt;8 hours&lt;br&gt;(08:00:00)&lt;/td&gt;
&lt;td&gt;2 days&lt;br&gt;(2-00:00:00)&lt;/td&gt;
&lt;td&gt;14 days&lt;br&gt;(14-00:00:00)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;scheduling-policy&#34;&gt;Scheduling Policy&lt;/h3&gt;
&lt;h4 id=&#34;backfill-strategy&#34;&gt;Backfill Strategy&lt;/h4&gt;
&lt;p&gt;The order of job admission onto compute nodes is determined through the configured scheduling policy. CBE SURM is configured for the &amp;ldquo;backfill&amp;rdquo; strategy.  Without backfill scheduling, each partition is scheduled strictly in priority order, which typically results in significantly lower system utilization and responsiveness than otherwise possible. Backfill scheduling will start lower priority jobs if doing so does not delay the expected start time of any higher priority jobs. Since the expected start time of pending jobs depends upon the expected completion time of running jobs, reasonably accurate time limits are important for backfill scheduling to work well.&lt;/p&gt;
&lt;p&gt;Slurm&amp;rsquo;s backfill scheduler takes into consideration every running job. It then considers pending jobs in priority order, determining when and where each will start, taking into consideration the possibility of resource requirements, etc. If the job under consideration can start immediately without impacting the expected start time of any higher priority job, then it does so. Otherwise the resources required by the job will be reserved during the job&amp;rsquo;s expected execution time.&lt;/p&gt;
&lt;p&gt;The backfill plugin will set the expected start time for pending jobs. A job&amp;rsquo;s expected start time can be seen using the &lt;code&gt;squeue --start command&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;prioritization&#34;&gt;Prioritization&lt;/h4&gt;
&lt;p&gt;The prioritisation configuration uses the multifactor prioritisation configuration and the following factors are taken into consideration to compute the sequence in which job are admitted onto partitions/computing resources and how pending jobs are sorted in the queue.&lt;/p&gt;
&lt;h5 id=&#34;factors&#34;&gt;Factors&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Factor&lt;/th&gt;
&lt;th&gt;What is it&lt;/th&gt;
&lt;th&gt;Explanation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Age&lt;/td&gt;
&lt;td&gt;the length of time a job has been waiting in the queue, eligible to be scheduled&lt;/td&gt;
&lt;td&gt;In general, the longer a job waits in the queue, the larger its age factor grows. However, the age factor for a dependent job will not change while it waits for the job it depends on to complete. Also, the age factor will not change when scheduling is withheld for a job whose node or time limits exceed the cluster&amp;rsquo;s current limits.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fair-Share (Usage)&lt;/td&gt;
&lt;td&gt;the fair-share factor serves to prioritize queued jobs such that those jobs charging accounts that are under-serviced are scheduled first, while jobs charging accounts that are over-serviced are scheduled when the machine would otherwise go idle.&lt;/td&gt;
&lt;td&gt;All jobs submitted at the time of this writing submit to the same account name (&amp;ldquo;root&amp;rdquo;) - this will change as soon as the cloud partition is in full configuration SLURM&amp;rsquo;s fair-share factor is a floating point number between 0.0 and 1.0 that reflects the shares of a computing resource that a user has been allocated and the amount of computing resources the user&amp;rsquo;s jobs have consumed. The higher the value, the higher is the placement in the queue of jobs waiting to be scheduled.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Partition&lt;/td&gt;
&lt;td&gt;a factor associated with each partition - see the partition section of this document&lt;/td&gt;
&lt;td&gt;it will favor partitions that have the bulk of the nodes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QOS&lt;/td&gt;
&lt;td&gt;a factor associated with each Quality Of Service - see the QOS section for the priority&lt;/td&gt;
&lt;td&gt;it will favor short running jobs that are submitted to the short qos&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jos Size (TRES)&lt;/td&gt;
&lt;td&gt;Size of the job based on Trackable Resources (TRES) such as Core, Memory, etc&lt;/td&gt;
&lt;td&gt;Within the same partition and qos favor small jobs&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5 id=&#34;formula&#34;&gt;Formula&lt;/h5&gt;
&lt;p&gt;All of the factors in this formula are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Job_priority =
    (PriorityWeightAge) * (age_factor) +
    (PriorityWeightFairshare) * (fair-share_factor) +
    (PriorityWeightJobSize) * (job_size_factor) +
    (PriorityWeightPartition) * (partition_factor) +
    (PriorityWeightQOS) * (QOS_factor) +
    SUM(TRES_weight_cpu * TRES_factor_cpu,
        TRES_weight_&amp;lt;type&amp;gt; * TRES_factor_&amp;lt;type&amp;gt;,
     ...)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The usage (fair share) can be retrieved by running the &lt;code&gt;slurm&lt;/code&gt; command with the option &lt;code&gt;shares&lt;/code&gt;
The priority of individual jobs can be trieved using the &lt;code&gt;sprio&lt;/code&gt; command for waiting jobs.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: RStudio on CLIP</title>
      <link>https://clip-hpc.github.io/docs/rstudio/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/rstudio/</guid>
      <description>
        
        
        &lt;p&gt;Some peopel have already been using RStudio on CBE through the XPRA service.
We now also support the native RStudio integration into CBE using the RStudio Server Pro solution.
You can access the web based RStudio at &lt;a href=&#34;https://rstudio.vbc.ac.at%5D&#34;&gt;https://rstudio.vbc.ac.at&lt;/a&gt;.  Once you login with your VBC credentials, you can create R session on CBE.&lt;br&gt;
You can specify the memory and cpu requirements and also select the SLURM partition the R session should run on. The job is submitted to the SLURM cluster with a maximum walltime of 8 hours (short queue).&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/rstudio/new-session_hu526be89e3fdda23f9af7f7ae3479c4ad_91896_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;459&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Figure 1 - One active session running and popup for creating a new session
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;One the job is schedued and runs a compute node you can open the R session and you will see the familiar RStudio interface.&lt;/p&gt;
&lt;p&gt;The RStudio session will support a subset of the R modues/versions that are supported on CBE. By default it should load the &lt;code&gt;r-bundle-bioconductor/3.8-foss-2018b-r-3.5.1&lt;/code&gt; module which comes with more than 600 R packages. You can switch to different supported versions of R in the GUI and by default the specific R version associated with a project will be loaded when opening the project.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/rstudio/r-versions_hub5659c79cd564770ac3cc252eadf1d6f_45585_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;419&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Figure 2 - Switching to a different R version
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;The default version of R can be changed by each user in the Global Options in the menubar &amp;ldquo;&lt;strong&gt;Tools &amp;ndash;&amp;gt; Global Options.. &amp;ndash;&amp;gt; General&lt;/strong&gt;&amp;quot;. If you are missing an R package, you can install it using the RStudio package manager. It will install it into your home folder.&lt;/p&gt;
&lt;p&gt;Additionally RStudio allows you to run code blocks as separate jobs. This way you can open a RStudio session with little memory and CPU requirements, develop your code/analysis and then submit the finished code block with higher resource requirements. The result of the job will be send back to your running RStudio session and shown in a separate window.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/rstudio/adhoc_hu442916810e03f14782f818a514b0a0d7_189172_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;426&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Figure 3 - Running RStudio session on compute node. Select Code and run it a separate job
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;Keep in mind that your job is executed as your USER and the logs of the R sessions and jobs can be found under &lt;code&gt;~/rstudio-logs&lt;/code&gt;. If you have no CBE cluster account yet, your RStudio session will fail so, &lt;a href=&#34;https://jira.vbc.ac.at/servicedesk/customer/portal/1/create/13&#34;&gt;please apply for cluster access at the helpdesk&lt;/a&gt;.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Interactive workloads</title>
      <link>https://clip-hpc.github.io/docs/cbe/interactive/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/cbe/interactive/</guid>
      <description>
        
        
        &lt;p&gt;To ensure short start times for interactive workloads, we have created a recurring reservation for such jobs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ReservationName=interactive StartTime=2020-03-18T09:00:00 EndTime=2020-03-18T17:00:00 Duration=08:00:00
   Nodes=clip-c1-[9-11],clip-c2-0 NodeCnt=4 CoreCnt=88 Features=(null) PartitionName=c Flags=FLEX,WEEKDAY,REPLACE,NO_HOLD_JOBS_AFTER_END
   TRES=cpu=88
   Users=(null) Accounts=gmi,imp,imba,vbcf,hephy,smi Licenses=(null) State=ACTIVE BurstBuffer=(null) Watts=n/a
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;strong&gt;name of the reservation&lt;/strong&gt; is “&lt;strong&gt;interactive&lt;/strong&gt;“. Jobs from interactive services (RStudio, XPRA, Jupyterhub) will launch on this reservation, but also sbatch / srun submitted jobs can be sent to it.&lt;/p&gt;


&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Important notes&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;the reservation is ACTIVE from 0900 to 1700, repeating every WEEKDAY.&lt;/li&gt;
&lt;li&gt;the reservation only contains nodes from &amp;ldquo;c&amp;rdquo; partition&lt;/li&gt;
&lt;li&gt;jobs started within the reservation will continue running once the reservation becomes inactive&lt;/li&gt;
&lt;li&gt;if enough idle resources are available jobs submitted to the reservation can also run on nodes not part of the reservation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;h2 id=&#34;how-to-submit-interactive-jobs&#34;&gt;How to submit interactive jobs&lt;/h2&gt;
&lt;p&gt;Keep your jobs as small as necessary (cpus, memory). Use &amp;ndash;time to set appropriate time limits for your job. Use this resource responsibly!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# example interactive job&lt;/span&gt;
srun -n &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt; --mem&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1g --time&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1:00:00 --reservation&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;interactive --pty bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will launch a job with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 task (core), 1 GB memory, that will expire after 1 hour.&lt;/li&gt;
&lt;li&gt;the resources will be taken from the reservation &amp;ldquo;&lt;code&gt;interactive&lt;/code&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--pty&lt;/code&gt; attaches the terminal to the first process in the job (the bash shell), i.e. you prompt will continue inside the allocated job&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: JupyterHub on CLIP</title>
      <link>https://clip-hpc.github.io/docs/jupyterhub/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/jupyterhub/</guid>
      <description>
        
        
        &lt;h2 id=&#34;basic-guide&#34;&gt;Basic Guide&lt;/h2&gt;
&lt;p&gt;For interactive data analysis we support JupyterHub on CBE.&lt;br&gt;
Our JupyterHub service allows you to spawn one or more JupyterLab sessions that are executed on the CBE compute nodes.   Inside this JupyterLab session the user can start Jupyter notebooks that either run an R or Python kernel and execute the code that is stored inside the notebooks. For more information about the Jupyter project refer to the &lt;a href=&#34;https://jupyter.org/&#34;&gt;official project website&lt;/a&gt;.
To access the JupyterHub service just open &lt;a href=&#34;https://jupyterhub.vbc.ac.at&#34;&gt;https://jupyterhub.vbc.ac.at&lt;/a&gt; in your browser and click on the &amp;ldquo;&lt;em&gt;&lt;strong&gt;Sign in with Azure AD&lt;/strong&gt;&lt;/em&gt;&amp;rdquo; button. Once authenticated the user will see the the screen to open the default JupyterLab session (see &lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;&lt;/em&gt;)
The user can choose from one of 3 job presets and also customize the individual job geometry by clicking on the &amp;ldquo;Customize&amp;rdquo; checkbox and changing the values for &amp;ldquo;&lt;em&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/em&gt;&amp;quot;, &amp;ldquo;&lt;em&gt;&lt;strong&gt;Cores&lt;/strong&gt;&lt;/em&gt;&amp;quot;, &amp;ldquo;&lt;em&gt;&lt;strong&gt;Runtime&lt;/strong&gt;&lt;/em&gt;&amp;quot;, &amp;ldquo;&lt;em&gt;&lt;strong&gt;Partition&lt;/strong&gt;&lt;/em&gt;&amp;rdquo; or &amp;ldquo;&lt;em&gt;&lt;strong&gt;GPUs&lt;/strong&gt;&lt;/em&gt;&amp;rdquo; (optional).&lt;/p&gt;
&lt;p&gt;Additionally the user can select from 2 Jupyter environments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Environment based on Conda (Python, R)&lt;/li&gt;
&lt;li&gt;Environment based on CBE env modules (Python 3.6.6)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following table should help choose the right option:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Features&lt;/th&gt;
&lt;th&gt;Conda enviornment&lt;/th&gt;
&lt;th&gt;CBE env module environment&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Kernels&lt;/td&gt;
&lt;td&gt;R &amp;amp; Python&lt;/td&gt;
&lt;td&gt;Python 3.6.6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Custom conda environment&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Support to load additional CBE modules&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Performance&lt;/td&gt;
&lt;td&gt;depends on pkgs installed in the conda environment&lt;/td&gt;
&lt;td&gt;optimized because they are compiled by HPC Team&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/jupyterhub/session_new_huaf787012deecdf4048a0adc826b72aff_150136_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;772&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Figure 1 - Creating a new JupyterLab session
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;Depending on which Jupyter environment is selected one of the following JupyterLab sessions will be presented to the user once the session is started.&lt;/p&gt;
&lt;table&gt;
&lt;tr&gt;
&lt;td&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 450px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/jupyterhub/session_conda_hu9cd03dedbd78ab64eeb753c470217c50_372428_440x0_resize_catmullrom_2.png&#34; width=&#34;440&#34; height=&#34;321&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Figure 2 - Conda based environment
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;  
&lt;/td&gt;
&lt;td&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 450px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/jupyterhub/session_modules_hub264fb07b975fd366bed83d49792d2e9_385208_440x0_resize_catmullrom_2.png&#34; width=&#34;440&#34; height=&#34;344&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Figure 3 - CBE env module based environment
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
On the left side, a navigation panel is displayed with different widgets:
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;File browser&lt;/strong&gt;: Open notebooks &amp;amp; create, delete files.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HDF5 viewer&lt;/strong&gt;: View HDF5 datasets in the main panel&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Session viewer&lt;/strong&gt;: See and manage (stop) running consoles and kernels&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Git viewer&lt;/strong&gt;: See git repository information&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Commands&lt;/strong&gt;: Quickly search and run comands that are also available through the top menu&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Data explorer&lt;/strong&gt;: Explore datasets (csv, etc)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tab viewer&lt;/strong&gt;: See open tabs and close them&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Software&lt;/strong&gt; (only in CBE env module environment): Load and unload Environment Modules&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the main panel you can start either a new Python, R notebook or console or open a terminal.&lt;/p&gt;
&lt;p&gt;If you started the Conda based Jupyter environment you will see in addition to the default Python3 notebook, also any additional conda environments that contain the ipykernel or irkernel package. Additionally you can access the conda package manager widget by choosing from the top navigation: &amp;ldquo;Settings -&amp;gt; Conda Packages Manager&amp;rdquo;:&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/jupyterhub/conda_pkg_manager_hud9c398296539012d062e0b1161a6dabe_383062_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;435&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Figure 4 - Conda Packages Manager
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;The &amp;ldquo;Conda Pacakges Manager&amp;rdquo; shows you the discovered conda environments and allows you to create new conda environments either from scratch or by cloning an existing one and installing/updating packages in any conda environment. Fore more information see the &lt;a href=&#34;https://github.com/fcollonval/jupyter_conda#jupyterlab&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;multiple-jupyterlab-sessions&#34;&gt;Multiple JupyterLab sessions&lt;/h2&gt;
&lt;p&gt;Sometimes you might want to run multiple different JupyterLab sessions - with different resource requirements or different Jupyter environments (Conda or CBE modules based) in parallel.
You can do this by opening the JupterHub view by either going through &amp;ldquo;&lt;strong&gt;File -&amp;gt; Hub Control Panel&lt;/strong&gt;&amp;rdquo; in the already opened JupyterLab session or by navigating to &lt;a href=&#34;https://jupyterhub.vbc.ac.at/hub/home&#34;&gt;https://jupyterhub.vbc.ac.at/hub/home&lt;/a&gt; in a new browser tab.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/jupyterhub/session_list_hu7812db3b2e491a243e2c4a4c2baf2117_141570_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;241&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
Figure 5 - Starting multiple JupyterLab sessions
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;Here you can see the list of existing JupyterLab sessions. You can stop or start and delete them. The default JupyterLab session can be controlled and accessed by the big &amp;ldquo;&lt;em&gt;&lt;strong&gt;Stop My Server&lt;/strong&gt;&lt;/em&gt;&amp;rdquo; and &amp;ldquo;&lt;em&gt;&lt;strong&gt;My Server&lt;/strong&gt;&lt;/em&gt;&amp;rdquo; buttons.&lt;br&gt;
To start a new JupyterLab session just choose a name and click on the &amp;ldquo;Add New Server&amp;rdquo; link.
This will show you the familiar session start screen from &lt;em&gt;&lt;strong&gt;Figure 1&lt;/strong&gt;&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&#34;custom-jupyter-kernels&#34;&gt;Custom Jupyter Kernels&lt;/h2&gt;
&lt;p&gt;A &amp;ldquo;&lt;em&gt;&lt;strong&gt;kernel&lt;/strong&gt;&lt;/em&gt;&amp;rdquo; in the Jupyter universe is an interpreter for a Notebook. Kernels are available for various languages (Python, R, &amp;hellip;). While Conda environments will be picked up automatically as custom kernels, you can also manually add Kernels to JupyterLab.&lt;/p&gt;
&lt;p&gt;In essence, the kernel spec is a json config file that tells JupyterLab which interpreter to use for the Notebook.&lt;/p&gt;
&lt;h3 id=&#34;example-python-virtualenv-as-kernel&#34;&gt;Example Python virtualenv as Kernel&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;create your virtual environment (i.e. &lt;code&gt;python3 -m venv my_experiment_venv&lt;/code&gt; )&lt;/li&gt;
&lt;li&gt;enable environment: &lt;code&gt;source my_experiment_venv/bin/activate&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The virtual environment needs to have the python module &amp;ldquo;&lt;em&gt;&lt;strong&gt;ipykernel&lt;/strong&gt;&lt;/em&gt;&amp;rdquo; installed (&lt;code&gt;pip install ipykernel&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;add the virtual environment as kernel to jupyter config:&lt;br&gt;
&lt;code&gt;python -m ipykernel install --user --name myenv --display-name &amp;quot;my Python (myenv)&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;The previous step will write some config files to &lt;code&gt;~/.local/share/jupyter/kernels/&amp;lt;myenv&amp;gt;/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;launch new session from Jupyterlab: new kernel is available as icon &amp;ldquo;&lt;em&gt;&lt;strong&gt;my Python (myenv)&lt;/strong&gt;&lt;/em&gt;&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To remove a kernel, you can delete the directory holding the kernel config files,i.e. &lt;code&gt;~/.local/share/jupyter/kernels/&amp;lt;myenv&amp;gt;/&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For more details, see the upstream docs: &lt;a href=&#34;https://ipython.readthedocs.io/en/latest/install/kernel_install.html#kernels-for-different-environments&#34;&gt;https://ipython.readthedocs.io/en/latest/install/kernel_install.html#kernels-for-different-environments&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;troubleshooting&#34;&gt;Troubleshooting&lt;/h2&gt;
&lt;h3 id=&#34;my-jupyterlab-session-does-not-start&#34;&gt;My JupyterLab session does not start&lt;/h3&gt;
&lt;p&gt;This can be due to multiple reasons.&lt;br&gt;
Make sure to start the new session with the &amp;ldquo;&lt;em&gt;&lt;strong&gt;enable logging $HOME/jupyterhub_{jobid}.log&lt;/strong&gt;&lt;/em&gt;&amp;rdquo; option turned on. If the session hangs, check the log file for any errors.&lt;br&gt;
Potentially your user environment is broken and the log files gives you hints how to fix it. If not create a ticket.&lt;/p&gt;
&lt;h3 id=&#34;i-created-a-new-conda-environment-but-i-cant-create-a-new-notebook-using-that-environment&#34;&gt;I created a new conda environment but I can&amp;rsquo;t create a new notebook using that environment&lt;/h3&gt;
&lt;p&gt;Make sure to install the &lt;code&gt;ipykernel&lt;/code&gt; or &lt;code&gt;irkernel&lt;/code&gt; into the new conda environment so that it will appear in the launcher and can be selected as a kernel in the kernel switcher. It might take a while until it shows up.&lt;/p&gt;
&lt;h3 id=&#34;my-python-notebook-does-not-start-when-i-use-the-cbe-env-module-based-environment&#34;&gt;My Python notebook does not start when I use the CBE env module based environment&lt;/h3&gt;
&lt;p&gt;Make sure that you didn&amp;rsquo;t accidentally unload the Python 3.6.6 CBE module in the lmod widget. You can also check the log file in your home directory what the error is.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Bulk-transfer of data</title>
      <link>https://clip-hpc.github.io/docs/bulktransfer/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/bulktransfer/</guid>
      <description>
        
        
        &lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;For transferring small files to and from CBE, users can use scp or sftp either from the command line or using some GUI applications such as &lt;a href=&#34;https://cyberduck.io/&#34;&gt;Cyberduck&lt;/a&gt; or &lt;a href=&#34;https://winscp.net/eng/download.php&#34;&gt;WinSCP&lt;/a&gt;. While the scp and sftp protocol work fine for smaller files ( &amp;lt; 1 GB), they are not an optimal solution for transferring large files ( up to TB). For those transfers we provide a dedicated Globus GridFTP service.&lt;br&gt;
The Globus GridFTP service allows you transfer large files either from your personal machine using the Globus Connect Personal or from another Globus GridFTP server by using a 3rd party copy/transfer.&lt;/p&gt;


&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;

Currently /scratch on CBE is available on the GridFTP server.
&lt;/div&gt;



&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;

Use GridFTP outside of VPN connections to achieve maximum transfer speeds.
&lt;/div&gt;

&lt;p&gt;To be able to use the GridFTP service following prerequisites are required:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Your account is enabled for CBE (open a &lt;a href=&#34;https://jira.vbc.ac.at/servicedesk/customer/portal/1/create/13&#34;&gt;ticket&lt;/a&gt; if it isn’t)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.globus.org/&#34;&gt;A globus.org&lt;/a&gt; account (see below how to create one)&lt;/li&gt;
&lt;li&gt;The &lt;a href=&#34;https://www.globus.org/globus-connect-personal&#34;&gt;Globus Connect Personal client&lt;/a&gt; installed on your laptop/workstation (if you want to transfer data from/to your laptop or workstation)&lt;/li&gt;
&lt;li&gt;&lt;em&gt;For 3rd party copy from a remote GridFTP server, you also need an account on the remote GridFTP server (optional)&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Documentation regarding the Globus system can be found here: &lt;a href=&#34;https://docs.globus.org/&#34;&gt;https://docs.globus.org/&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;creating-a-globusorg-account&#34;&gt;Creating a globus.org account&lt;/h2&gt;
&lt;p&gt;In order to use the Globus system, you need to have an account with globus.org. Open &lt;a href=&#34;https://www.globus.org&#34;&gt;https://www.globus.org&lt;/a&gt; in your browser and click on the &lt;a href=&#34;https://app.globus.org/&#34;&gt;login button&lt;/a&gt; on the top right corner. As you probably don’t have an existing account, it will ask you to create a new account or use an existing organizational account. Globus currently doesn’t support our organizational accounts, so you need to create a new globus.org account.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/globus-account_hud87be994b4c335a0e300757a10884886_272791_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;702&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;You can do this in 2 ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Implicitly by authenticating with your existing Google or ORCID account&lt;/li&gt;
&lt;li&gt;Explicitly by creating a Globus ID  account in case you don’t want to link and use your existing Google/ORCID account with your new globus.org account (you can still do this later on)&lt;/li&gt;
&lt;/ol&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/create-account_hubda94bca2cdf1f9a91ac965eabbe3ba4_308777_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;844&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;Once you created your globus.org account, you will receive an email with a link to validate your email account. You need to open the link once in your browser to activate your globus.org account. As soon as this is done you can go back to &lt;a href=&#34;https://globus.org&#34;&gt;https://globus.org&lt;/a&gt; and click on the &lt;a href=&#34;https://app.globus.org/&#34;&gt;login link&lt;/a&gt; in the top right corner. If you are not automatically logged in, choose the authentication provider that you used before to create the account (Google/ORCID or Globus ID).&lt;br&gt;
If you want to transfer files from and to your laptop/workstation, continue with the next chapter&lt;/p&gt;
&lt;h2 id=&#34;install-globus-connect-personal-client&#34;&gt;Install Globus Connect Personal Client&lt;/h2&gt;
&lt;p&gt;To be able to transfer data from and to your laptop, you need to install the Globus Connect Personal Client. This client will run in the background similar to the Dropbox, Google Drive client and take care of the actual data transfer. Each Globus Personal Connect client is an endpoint in Globus similar to our GridFTP server.&lt;/p&gt;


&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Multiple endpoints&lt;/h4&gt;
Currently /scratch on CBE is available on the GridFTP server.
&lt;/div&gt;

&lt;p&gt;The first time you log into &lt;a href=&#34;https://app.globus.org/&#34;&gt;https://app.globus.org/&lt;/a&gt; you will see following screen:&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-globus_hu8da48505165ddb0a1b6216ca7d04dcb9_100011_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;433&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;Here you can create a new Globus Connect Personal Client endpoint and download the client.
You need two specify the name for your endpoint (i.e. work-mac) and then click on &amp;ldquo;&lt;strong&gt;Generate Setup Key&lt;/strong&gt;&amp;quot;. This key will be later used to authenticate your client with Globus. Copy it from the popup and keep it safe. You will need it for the client installation (next step).
Depending on your operating system, you need to download the corresponding client. See the matching installation guide for more information.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Windows: &lt;a href=&#34;https://docs.globus.org/how-to/globus-connect-personal-windows/&#34;&gt;https://docs.globus.org/how-to/globus-connect-personal-windows/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mac:  &lt;a href=&#34;https://docs.globus.org/how-to/globus-connect-personal-mac/&#34;&gt;https://docs.globus.org/how-to/globus-connect-personal-mac/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linux: &lt;a href=&#34;https://docs.globus.org/how-to/globus-connect-personal-linux/&#34;&gt;https://docs.globus.org/how-to/globus-connect-personal-linux/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once the installation is finished your Globus Personal Connect client will appear as an endpoint in the &lt;a href=&#34;https://app.globus.org/endpoints?scope=administered-by-me&#34;&gt;endpoint list&lt;/a&gt;:&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-endpoints_hue4e1ab3c183d8a98a93f42eadf11104f_73698_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;202&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;Additionally on Mac and Windows the Globus Personal Connect Client will be visible in the task bar/menu bar&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-notification_hue3d7a0f615a7db1e81b24ee28b808900_311989_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;306&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;h2 id=&#34;transfer-data-between-pclaptop-and-cbescratch&#34;&gt;Transfer data between PC/laptop and CBE/scratch&lt;/h2&gt;
&lt;p&gt;To transfer data from /scratch to your laptop or from your laptop to /scratch you need to open the Globus file-manager in your browser:  &lt;a href=&#34;https://app.globus.org/file-manager&#34;&gt;https://app.globus.org/file-manager&lt;/a&gt;.&lt;/p&gt;


&lt;div class=&#34;alert alert-warning&#34; role=&#34;alert&#34;&gt;

Make sure that the Globus Connect Personal Client is running in the background and connected (see screenshot above)
&lt;/div&gt;








&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-filemanager_hu1f68237aaa18506eb7950bd1e897689f_161776_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;434&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;The page is split vertically in two file explorers. Now you need to select the two endpoints, our GridFTP server on the one side and your Globus Personal Connect endpoint (laptop) on the other side. You can do this by clicking on the Collection searchbox. This will show your recently used endpoints.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-collection_hu87792abc4e0c536c57a1fe12ba0aedd3_83021_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;358&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;It is probably empty. Just type in Vienna BioCenter in the textbox next to “&lt;strong&gt;Collection&lt;/strong&gt;”  and it should display our GridFTP server “Vienna BioCenter”&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-biocenter_hub20462bd57012c18a93d3e4b13a860c9_43110_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;138&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;Once you select the endpoint, you need to login with your Campus credentials.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-credentials_huc9b1e231d25a54920d2af8c15c706d9f_91865_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;430&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;After that you will see the directory listing of &lt;code&gt;/scratch&lt;/code&gt;. You can navigate into the folders and select files.
To transfer files from and to your laptop, you need to select your Globus Personal Connect endpoint on the other side. Again you click on the searchbox next to &amp;ldquo;&lt;strong&gt;Collection&lt;/strong&gt;&amp;rdquo; and you can either search for the name of your Globus Personal Connect endpoint or click on the “Your collection tab” where all your Globus Personal Connect Client endpoints should appear.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-personal_hub0fb6c8be9e7d2e62bac37b57e71e63c_38314_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;214&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;Once you select it, it shows you the directory listing on your laptop:&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-listing_hu810f53321d9acc96c27fec69f06d7aaf_125959_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;431&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;To transfer files from your laptop you select the first on the right side and then click on “&lt;strong&gt;Transfer/Sync to&lt;/strong&gt;” and select the folder where you want the file to be transferred to on the left side and then click on the &amp;ldquo;&lt;strong&gt;Start&lt;/strong&gt;&amp;rdquo; bottom on the bottom right side. You will see a notification about the started transfer and if you click on “View details” you will see the progress of the transfer.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-transfer_hudde56fcf57e4132084bc2f58db843da8_133907_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;336&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;You can also get there by clicking on the “&lt;a href=&#34;https://app.globus.org/activity&#34;&gt;Activity&lt;/a&gt;” tab on the left sidebar. There you can see all your transfers.


&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;

Once the transfer is finished you will also receive an email.
&lt;/div&gt;

Copying files from /scratch to your laptop works the same way.&lt;/p&gt;
&lt;h2 id=&#34;3rd-party-transfers&#34;&gt;3rd party transfers&lt;/h2&gt;
&lt;p&gt;If you have an account on a remote GridFTP server of a different institute/organisation, you can initiate a 3rd party copy between both endpoints. This basically works the same way as transferring files from/to your laptop.
Instead of your Personal Connect Endpoint you select the GridFTP endpoint of the other institute/organization where you have to login with the credentials provided to you by the organization/institute. The copying steps are the same as before.&lt;/p&gt;


&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;

The computer from which you initiated the transfer is not in the data path and can be safely closed shutdown. The copying will continue between the two endpoints.
&lt;/div&gt;








&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 850px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/bulktransfer/app-3rdparty_hu254dbf0a41a74878ce44e7167d3ab3e3_133174_840x0_resize_catmullrom_2.png&#34; width=&#34;840&#34; height=&#34;429&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;p&gt;In the screenshot you can see the file-manager with the Vienna BioCenter GridFTP endpoint opened on one side and the Globus Tutorial Endpoint 1 (public test GridFTP server from Globus) on the other side.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Change Notes</title>
      <link>https://clip-hpc.github.io/docs/changelog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/changelog/</guid>
      <description>
        
        
        

&lt;div class=&#34;alert alert-primary&#34; role=&#34;alert&#34;&gt;

CBE maintenance is scheduled on the first Thursday of every month from 10 a.m. until 10 a.m. the next day. A reservation will be placed on the entire system so that all job queues will be empty when the system goes down. Scheduled maintenance periods will be announced in the Message of the Day (MOTD) when logging into CBE ahead of time as well as via e-mail.
&lt;/div&gt;

&lt;h2 id=&#34;2020-04-16&#34;&gt;2020-04-16&lt;/h2&gt;
&lt;h3 id=&#34;rstudio&#34;&gt;Rstudio&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Update to version RStudio Server Pro 1.2.5033-1, &amp;ldquo;Orange Blossom&amp;rdquo; (b80dc554).&lt;/li&gt;
&lt;li&gt;Rstudio session will no longer read ~/.bashrc or ~/.bash_profile.&lt;/li&gt;
&lt;li&gt;Support for multiple versions of R (currently 3.5.1 and 3.6.0, bare or with texlive+bioconductor)&lt;/li&gt;
&lt;li&gt;The default R version can be changed by the user and is remembered when restoring an R project&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2020-03-18&#34;&gt;2020-03-18&lt;/h2&gt;
&lt;h3 id=&#34;interactive-reservation&#34;&gt;interactive reservation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;added a recurring reservation for interactive jobs (Mon-Fri 0900-1700)&lt;/li&gt;
&lt;li&gt;RStudio, Jupyterhub and XPRA jobs will submit on this reservation&lt;/li&gt;
&lt;li&gt;See documentation: Interactive workloads&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;2019-12-05&#34;&gt;2019-12-05&lt;/h2&gt;
&lt;h3 id=&#34;xpra&#34;&gt;XPRA&lt;/h3&gt;
&lt;p&gt;Web-based graphical cluster access via XPRA is now available on CBE, similar to the II-2 cluster before. Sessions can be launched from the &lt;a href=&#34;https://it.vbc.ac.at/clip/cbe/xpra&#34;&gt;IT website&lt;/a&gt; while some documentation is available &lt;a href=&#34;https://docs.vbc.ac.at/books/scientific-computing/page/web-access-%28xpra%29-b1a&#34;&gt;here&lt;/a&gt;. Currently available applications include Rstudio, Fiji, IGV Genome Browser and generic XTerm. The latter of course permits the invocation of other tools with graphical user interfaces like SPHIRE or Relion after loading the respective modules. Note that CBE access is required to run XPRA sessions, for which you can apply &lt;a href=&#34;https://jira.vbc.ac.at/servicedesk/customer/portal/1/create/13&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;rstudio-1&#34;&gt;Rstudio&lt;/h3&gt;
&lt;p&gt;As mentioned above, access to Rstudio is provided through XPRA. Please see the Links in the paragraph above. Rstudio documentation is available here.&lt;/p&gt;
&lt;h3 id=&#34;jupyterhub&#34;&gt;JupyterHub&lt;/h3&gt;
&lt;p&gt;JupyterHub has also been installed recently and can be accessed here. Users can specify job types with different resource configurations (memory, GPUs, runtime) and choose from a Conda-based environment for interactive work or an environment based on CBE’s Python modules for prototyping batch jobs. Note that CBE access is required to use JupyterHub, for which you can apply here.&lt;/p&gt;
&lt;h3 id=&#34;singularity&#34;&gt;Singularity&lt;/h3&gt;
&lt;p&gt;The Singularity module available on CBE was upgraded from version 3.2.1 to version 3.4.1. The earlier version is no longer available. Please remember to update your scripts accordingly. Also note that due to a software bug, we had to unset $SINGULARITY_TMPDIR which used to point to your user home directory in the previous version.
Building custom Singularity images is now facilitated with a dedicated Jenkins build pipeline and our own Singularity registry to which the images are pushed. As individual setup is necessary, interested users are kindly asked to open an HPC ticket here in order to make an appointment.&lt;/p&gt;
&lt;h2 id=&#34;2019-10-03&#34;&gt;2019-10-03&lt;/h2&gt;
&lt;h3 id=&#34;qos-limits&#34;&gt;QoS Limits&lt;/h3&gt;
&lt;p&gt;The QoS Limits for the GPU ‘g’ partition have been adjusted now to include all nodes with GPUs for medium, long and short. The effectively means that the limits are set to 100% of the resources. We will adjust (lower) the limits once more applications on the GPU nodes are running.&lt;/p&gt;
&lt;p&gt;The QoS Limits for large memory nodes ‘m’ have been adjusted now to include all the memory available in the medium, long and short QoS. This will be adjusted on a per need basis, but since these machines are designed for large memory challenges it makes more sense to not limit the resources for the time being.&lt;/p&gt;
&lt;h3 id=&#34;login-nodes&#34;&gt;Login Nodes&lt;/h3&gt;
&lt;p&gt;The individual login nodes that are round-robined when SSHing into &lt;code&gt;cbe.vbc.ac.at&lt;/code&gt; have now properly exposed DNS names (relevant for reconnecting to your tmux session et al) and  are reachable as &lt;code&gt;clip-login-{0,1}.cbe.vbc.ac.at&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;environment-modules&#34;&gt;Environment Modules&lt;/h3&gt;
&lt;p&gt;An issue that involved resetting the &lt;code&gt;$MODULEPATH&lt;/code&gt; environment variable causing the list of available modules to be empty upon running a login shell either through tmux or manually was resolved and modules are properly initialised and usable from login shells and/or tmux sessions.&lt;/p&gt;
&lt;h3 id=&#34;announcement-of-maintenance&#34;&gt;Announcement of Maintenance&lt;/h3&gt;
&lt;p&gt;We are now rendering the upcoming maintenance windows and reservations on the machine in the Message of the Day (MOTD) upon login. For the next windows we will announce this explicitly trough email in addition to the MOTD.&lt;/p&gt;
&lt;h3 id=&#34;singularity-1&#34;&gt;Singularity&lt;/h3&gt;
&lt;p&gt;Due to a security vulnerability in the Singularity container run time environment the version and the module for 3.1.0 have been removed from the system and the only usable version in place is 3.2.0.&lt;/p&gt;
&lt;p&gt;The Singularity &lt;code&gt;$SINGULARITY_TMPDIR&lt;/code&gt; variables now defined system wide to point to your home directory for every user, this should remove some of the potential issues involved in converting singularity image formats.&lt;/p&gt;
&lt;h3 id=&#34;software-trees&#34;&gt;Software Trees&lt;/h3&gt;
&lt;p&gt;The system now offers two additional software trees named &lt;code&gt;build-env/i2020&lt;/code&gt; and &lt;code&gt;build-env/f2020&lt;/code&gt; – from now on all software trees will be prefix’ed with either ‘I’ or ‘f’ to indicate whether they are based on the Intel compiler toolchain (‘I’) or on the GNU gcc based toolchain (‘f’). &lt;code&gt;The build-env/i2020&lt;/code&gt; contains the commercial Intel compiler, Intel MPI, Intel accelerated numerical libraries (MKL). You can easily switch between software trees by loading the appropriate build-env module. The existing and default software tree stays in place as is.&lt;/p&gt;

      </description>
    </item>
    
  </channel>
</rss>
