<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CLIP – Compute Batch Environent (CBE)</title>
    <link>https://clip-hpc.github.io/docs/cbe/</link>
    <description>Recent content in Compute Batch Environent (CBE) on CLIP</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	  <atom:link href="https://clip-hpc.github.io/docs/cbe/index.xml" rel="self" type="application/rss+xml" />
    
    
      
        
      
    
    
    <item>
      <title>Docs: Application and Software on CBE</title>
      <link>https://clip-hpc.github.io/docs/cbe/software/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/cbe/software/</guid>
      <description>
        
        
        &lt;h2 id=&#34;environment-modules&#34;&gt;Environment Modules&lt;/h2&gt;
&lt;p&gt;The Modules package provides for dynamic modification of a user&amp;rsquo;s environment with module files. Each module file contains the information needed to configure the shell for an application. Once the package is initialized, the environment can be modified dynamically on a per-module basis using the module command that interprets module files. Typically, module files instruct the module command to alter or set shell environment variables, such as PATH or MANPATH. Module files may be shared by many users on a system and users may have their own collection to supplement or replace the shared module files.&lt;/p&gt;
&lt;p&gt;The Modules environment management package offers support for dynamic modification of the user environment via modules. Each module contains all the information needed to configure the shell for a particular application. To make major changes in your user environment, such as switching to a different compiler or a different version of a library, use the appropriate Modules commands to select the desired modules.&lt;br&gt;
The advantage in using Modules is that users are not required to specify explicit paths (&lt;code&gt;$PATH&lt;/code&gt; or &lt;code&gt;$LD_LIBRARY_PATH&lt;/code&gt;) for different executable versions or to set the &lt;code&gt;$MANPATH&lt;/code&gt; and other environment variables manually. Instead, all the information required in order to use a given piece of software is embedded in the module and set automatically when you load the module.&lt;/p&gt;
&lt;p&gt;In general, the simplest way to make certain that the elements of an applications environment function correctly together is by using the modules software and trusting it to keep track of paths and environment variables, and avoiding embedding specific directory paths into startup and job scripts.&lt;/p&gt;
&lt;p&gt;For CBE the environment modules package available across the compute infrastructure is called Lmod and provides an implementation of the environment modules  in the scripting language Lua. It understands classical module files as written in the Tcl language and module files written in Lua and has many benefits over the classic environment modules whoever all files are written in Lua.&lt;/p&gt;
&lt;h3 id=&#34;environment-modules-package-commands&#34;&gt;Environment Modules Package Commands&lt;/h3&gt;
&lt;p&gt;The following table gives an overview of available module-commands:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ml load &lt;code&gt;module-name&lt;/code&gt;/&lt;code&gt;version&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Loads a given module file and modifies shell accordingly.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml help&lt;/td&gt;
&lt;td&gt;Displays a list of all available module commands&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml unload &lt;code&gt;module-name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Unload a given module and removes shell modification&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml whatis &lt;code&gt;module-name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Displays a short description of the software and home page of the package&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml display &lt;code&gt;module-name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Show all variables set by the module and see what is going to be changed in the current shell session&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml&lt;/td&gt;
&lt;td&gt;Shows a list of loaded modules in the current session&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml av&lt;/td&gt;
&lt;td&gt;Displays a list of all available modules in the system&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml swap &lt;code&gt;module-name&lt;/code&gt; &lt;code&gt;module-name&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Swaps the module with another module (unload/load)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml purge&lt;/td&gt;
&lt;td&gt;Remove all loaded modules from the current session&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml use &lt;code&gt;path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Add a given path and as an additional path to look for module files&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ml unuse &lt;code&gt;path&lt;/code&gt;&lt;/td&gt;
&lt;td&gt;Remove a path from the search path for modules&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;software-trees&#34;&gt;Software Tree(s)&lt;/h2&gt;
&lt;p&gt;IT service install highly optimized software in  a CPU specific way. All optimizations options for the CPUs of the compute nodes are selected.&lt;/p&gt;
&lt;p&gt;The software trees are published under the &lt;code&gt;/software&lt;/code&gt; directory and the system&amp;rsquo;s default configuration picks up one and only one tree. Mixing modules across different trees is not supported and should be left to expert users.&lt;/p&gt;
&lt;h2 id=&#34;lifecycle-policy-of-installed-software&#34;&gt;Lifecycle Policy of Installed Software&lt;/h2&gt;
&lt;p&gt;The initial system contains one software tree installed under path &lt;code&gt;/software/&lt;/code&gt;. Application software will never be reinstalled with different features and versions are never removed. Once a new software tree is and set as the system&amp;rsquo;s default software tree  users will be notified. Older software trees will stay in place and will not be removed during the life time of the system.&lt;/p&gt;
&lt;h2 id=&#34;module-names&#34;&gt;Module Names&lt;/h2&gt;
&lt;p&gt;The module contain not only the name of the application but also specify which compiler (toolchain) was used to compile them. If no compiler toolchain is specified then the Linux distributions base installation compiler was used for the compilation. This is usually done on software that is either used to build other software or where optimizations is of no concern.&lt;/p&gt;


&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Lowercase&lt;/h4&gt;
Please note that the module naming scheme is all lower case for all installed software.
&lt;/div&gt;

&lt;h2 id=&#34;compiler-toolchains&#34;&gt;Compiler Toolchains&lt;/h2&gt;
&lt;p&gt;Are basically a (set of) compilers together with a set of libraries that provide additional support functionality that is commonly required to build software to satisfy build-time dependencies. In the world of High Performance Computing this usually consists of an library for MPI (inter-process communication over a network), BLAS/LAPACK (linear algebra routines) and FFT (Fast Fourier Transforms) as the most basic &amp;ldquo;toolset&amp;rdquo;. If a piece of software does not benefit from or is not implemented on top of a BLAS library or is not an MPI program then this functionality does not affect the program in any sense.&lt;/p&gt;
&lt;p&gt;In general toolchains cannot be mixed, usually not even in different version and require a recompilation of all the software that needs to be compiled with a newer version of the toolchain. This is mostly true for software written in C++ and Fortran, all MPI packages and is generally irrelevant for programs in C, Python, etc.&lt;/p&gt;
&lt;p&gt;Programming languages written on top of C++ or Fortran like R that have binary dependencies that need compilation are dependant on the compiler and version for the non R parts. The same is true for Python software using binary libraries implemented in the mention run times.&lt;/p&gt;
&lt;p&gt;IT services will generally provide relatively recent version of the GNU programming environment and the commercial Intel programming environment. It is essential that software and all of it&amp;rsquo;s dependencies are available for the application that don&amp;rsquo;t depend on their underlying operating system environment to have the dependencies and all the needed version available.&lt;/p&gt;
&lt;p&gt;In general all tools are either compiled with the FOSS toolchain or the INTEL toolchain and the toolchain is instantiated once a module is loaded that was compiled via one or the other toolchain as a dependency.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Toolchain&lt;/th&gt;
&lt;th&gt;Components&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;th&gt;Use Cases&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;foss/2018b&lt;/td&gt;
&lt;td&gt;GCC (c/c++, fortran,&amp;hellip;), OpenMPI, OpenBLAS, ScLAPACK, FFTW&lt;/td&gt;
&lt;td&gt;Open source toolchain in the 2018b version based on GCC 8.0 and OpenMPI 3.0.2&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;Toolchain for general purpose use&lt;/li&gt;&lt;li&gt;High performance&lt;/li&gt;&lt;li&gt;Based on free and open source software (FOSS)&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Storage on CBE</title>
      <link>https://clip-hpc.github.io/docs/cbe/storage/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/cbe/storage/</guid>
      <description>
        
        
        &lt;p&gt;The CBE cluster has several IMP/IMBA/GMI storage devices and clusters attached to it and mounted within the file system. This consists of devices for storing scientific data, the software trees and user home directories specific to CBE and a global parallel shared file system between the CBE nodes.&lt;/p&gt;
&lt;p&gt;The Isilon clusters (NFS) are globally available for storing group data and an site wide scratch file system available on all clusters, workstations and other devices connected to the IMP/IMBA/GMI network.&lt;/p&gt;
&lt;p&gt;The GMI NetApp file servers are globally available for storing group data and an site wide scratch file system available on all clusters, workstations and other devices connected to the IMP/IMBA/GMI network.&lt;/p&gt;
&lt;p&gt;In addition to this a very high performance file system  (BeegFS) is implemented within CBE&amp;rsquo;s flash/SSD compute nodes that offers and aggregated storage from all the individual SSDs within those nodes to all other compute nodes.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NetApp Science Storage&lt;br&gt;(/scratch and /resources)&lt;/th&gt;
&lt;th&gt;NetApp Infra Storage&lt;br&gt;(/users)&lt;/th&gt;
&lt;th&gt;CBE Fast Scratch&lt;br&gt;(/scratch-cbe)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;clustertmp mounted on /scratch and on the compute nodes and all user facing nodes&lt;/li&gt;&lt;li&gt;All compute clusters (except MENDEL) have this storage mounted&lt;/li&gt;&lt;li&gt;groups mounted on /groups on all&lt;/li&gt;&lt;li&gt;compute nodes and all user facing nodes&lt;/li&gt;&lt;li&gt;resources mounted on /resources on all compute nodes and all user facing nodes&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;CBE specific home directories on /users&lt;/li&gt;&lt;li&gt;GMI specific projects are mounted under /groups/&amp;lt;group-name&amp;gt;/projects/project-name&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;/scratch-cbe on all compute nodes from local SSD flash drives on the SSD flash nodes&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;storage-location-use-cases&#34;&gt;Storage Location Use Cases&lt;/h2&gt;
&lt;p&gt;The following table gives an overview of available module-commands:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Storage Location&lt;/th&gt;
&lt;th&gt;Use case&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;/scratch-cbe&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;storage of data and files required by jobs and during job execution&lt;/li&gt;&lt;li&gt;highest level of performance independent of IO profile&lt;/li&gt;&lt;li&gt;automatically purged after 30 days&lt;/li&gt;&lt;li&gt;copy data from /scratch  or /groups to /scratch-cbe and back for high performance access&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/scratch&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;storage of all bulk scientific temporary data that is generated while jobs are running that is of general purpose nature&lt;/li&gt;&lt;li&gt;storage of all files that require high performance access (preferably sequential IO)&lt;/li&gt;&lt;li&gt;automatically purged after 30 days &lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/groups/&amp;lt;group-name&amp;gt;/projects&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;GMI &amp;ldquo;projects folder&amp;rdquo; mounted into the groups hierarchy&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/groups and /resources&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;storage all long term data&lt;/li&gt;&lt;li&gt;avoid processing data on this location due to performance penalties (1:8 ratio against clustertmp) for older files&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/users&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;storage of configuration files and program files&lt;/li&gt;&lt;li&gt;do not keep data here&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;/tmp on compute nodes&lt;/td&gt;
&lt;td&gt;&lt;ul&gt;&lt;li&gt;Redirected to /scratch-cbe/tmp-inst/&amp;lt;user-name&amp;gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;h2 id=&#34;staging-data&#34;&gt;Staging Data&lt;/h2&gt;
&lt;p&gt;The fast parallel BeeGFS file system hosting (mainly) &lt;code&gt;/scratch-cbe&lt;/code&gt;, as the mount point implies, is intended purely as a scratch space for fast access during compute jobs. Users are supposed to copy the respective data to &lt;code&gt;/scratch-cbe&lt;/code&gt; shortly before computing and move results out of &lt;code&gt;/scratch-cbe&lt;/code&gt; afterwards. This is called staging. The 30 days lifetime are merely a grace period in this usage scenario.&lt;/p&gt;
&lt;p&gt;To assist in implementing of a suitable staging practice with your compoute jobs on CLIP/CBE, a ready-made staging script is provided.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;user.name@clip-login-0 ~&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;$ stage --help
usage: /software/system/utils/stage source_folder destination_folder
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This script will spawn single copy jobs on 20 compute nodes in parallel in order to maximize throughput. Of course, you can always use this script to copy your data in and out of &lt;code&gt;/scratch-cbe&lt;/code&gt; manually. Also, when used with a single batch job or a job array, you can easily insert staging commands at the beginning and end of your batch script.&lt;/p&gt;
&lt;p&gt;When you intend to use the staged data in multiple independent compute jobs, it is advisable to create separate staging jobs and link them before and after the actual compute jobs. This can be done via the &lt;code&gt;--dependency&lt;/code&gt; parameter to the &lt;code&gt;sbatch&lt;/code&gt; command. Please see the sbatch man page for details or &lt;a href=&#34;https://slurm.schedmd.com/sbatch.html&#34;&gt;click here&lt;/a&gt; for a web version.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Advanced SLURM Guide</title>
      <link>https://clip-hpc.github.io/docs/cbe/slurm/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/cbe/slurm/</guid>
      <description>
        
        
        &lt;h2 id=&#34;slurm&#34;&gt;SLURM&lt;/h2&gt;
&lt;p&gt;As a cluster resource manager (CPUs, memory, custom resources like SSDs, etc.), SLURM has three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a job) on the set of allocated node or for parallel jobs on nodes. Finally, it arbitrates conflicting requests for resources by managing a queue of pending work.&lt;/p&gt;
&lt;h3 id=&#34;slurm-terminology&#34;&gt;SLURM Terminology&lt;/h3&gt;
&lt;p&gt;The entities managed by these SLURM include nodes, the compute resource in SLURM, &lt;em&gt;partitions&lt;/em&gt;, which group nodes into logical sets, &lt;em&gt;jobs&lt;/em&gt;, or allocations of resources assigned to a user for a specified amount of time, and &lt;em&gt;job steps&lt;/em&gt;, which are sets of (possibly parallel) tasks within a job. Each job in the priority-ordered queue is allocated nodes within a single partition. Nodes can be members of more than node partition.&lt;/p&gt;
&lt;p&gt;Once a job is assigned one node or a set of nodes, the user is able to initiate work in the form of job steps in any configuration within the allocation. For instance, a single job step may be started that utilizes all nodes allocated to the job if supported by the software, or several job steps may independently use a portion of the allocation linke single or multiple cores on one compute node.&lt;/p&gt;
&lt;h3 id=&#34;slurm-command-line-utilities&#34;&gt;SLURM Command Line Utilities&lt;/h3&gt;
&lt;p&gt;Users interact with SLURM through command line utilities. In this section we just introduce the most important commands available to users while the use of these will be covered in a later section of this document.&lt;/p&gt;
&lt;p&gt;Please note that on CBE the BASH shell is configured to tab-complete the options available to these SLURM command line utilities.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Command&lt;/th&gt;
&lt;th&gt;Use Case&lt;/th&gt;
&lt;th&gt;Notes&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;srun&lt;/td&gt;
&lt;td&gt;submitting jobs for execution and optionally controlling it interactively&lt;/td&gt;
&lt;td&gt;&lt;code&gt;srun&lt;/code&gt; is the user interface to accessing resources managed by SLURM. Users may utilize &lt;code&gt;srun&lt;/code&gt; to allocate resources, submit batch jobs, run jobs interactively, attach to currently running jobs, or launch a set of parallel tasks (job step) for a running job.&lt;br&gt;&lt;code&gt;srun&lt;/code&gt; can either create an on demand resource selection before starting a possibly interactive command or launch tasks into an resource allocation crated by running salloc&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scancel&lt;/td&gt;
&lt;td&gt;terminating a pending or running job.&lt;/td&gt;
&lt;td&gt;&lt;code&gt;scancel&lt;/code&gt;terminates queued jobs or signals running jobs or job steps. The default signal is &lt;code&gt;SIGKILL&lt;/code&gt;, which indicates a request to terminate the specified job or job step. &lt;code&gt;scancel&lt;/code&gt; identifies the job(s) to be signaled through user specification of the SLURM job id, job step id, user name, partition name, and/or job state. If a job id is supplied, all job steps associated with the job are affected as well as the job and its resource allocation. If a job step id is supplied, only that job step is affected. scancel can only be executed by the job’s owner or a privileged user.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;squeue&lt;/td&gt;
&lt;td&gt;monitoring job queues&lt;/td&gt;
&lt;td&gt;&lt;code&gt;squeue&lt;/code&gt; reports the state of SLURM jobs. It can filter these jobs input specification of job state (RUN, PENDING, etc.), job id, user name, job name, etc. If no specification is supplied, the state of all pending and running jobs is reported. squeue also has a variety of sorting and output options&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sinfo&lt;/td&gt;
&lt;td&gt;show a summary of partition and node information&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sbatch&lt;/td&gt;
&lt;td&gt;submit a job script for later execution. The script will typically contain one or more srun commands to launch parallel tasks&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;salloc&lt;/td&gt;
&lt;td&gt;allocate resources and potentially use them for srun&lt;/td&gt;
&lt;td&gt;allocates resources for a job in real time. Typically this is used to allocate resources and spawn a shell. The shell is then used to execute srun commands to launch serial or parallel tasks&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scontrol&lt;/td&gt;
&lt;td&gt;retrieve SLURM information on partitions, limits, jobs, etc&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sattach&lt;/td&gt;
&lt;td&gt;attaches the terminal stdin/stdout to the job&amp;rsquo;s stdin/stdout&lt;/td&gt;
&lt;td&gt;sattach allows a terminal to attach/reattach to a job&amp;rsquo;s output to monitor it&amp;rsquo;s output/errors and progression&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sprio&lt;/td&gt;
&lt;td&gt;reports priority of queueing jobs&lt;/td&gt;
&lt;td&gt;list output of individual components that affected priority/order in queue for waiting jobs&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;custom-vbc-slurm-utilities&#34;&gt;Custom VBC SLURM Utilities&lt;/h3&gt;
&lt;p&gt;The following utilities are provided in addition to the SLURM commands as convenient shortcuts for quicker access to relevant information on the queue&amp;rsquo;s status, available and used resources, limits configured etc.&lt;/p&gt;
&lt;h4 id=&#34;slurm-ltcommandgt&#34;&gt;slurm &amp;lt;command&amp;gt;&lt;/h4&gt;
&lt;p&gt;Show status of partitions, jobs, node usage history, priorties etc.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;john.doe@clip-login-0 ~&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;$ slurm 

Show or watch job queue:
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; queue     show own &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; q &amp;lt;user&amp;gt;  show user&lt;span style=&#34;color:#a40000&#34;&gt;&amp;#39;&lt;/span&gt;s &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; quick     show quick overview of own &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; shorter   sort and compact entire queue by job size
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; short     sort and compact entire queue by priority
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;watch&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; full      show everything
 slurm &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;w&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;q&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;qq&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;ss&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;s&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;f&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt; shorthands &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;for&lt;/span&gt; above!

 slurm qos               show job service classes
 slurm top &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;queue&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;all&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;   show summary of active users

Show detailed information about jobs:
 slurm prio &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;all&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;short&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;  show priority components
 slurm j&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;job &amp;lt;jobid&amp;gt;     show everything &lt;span style=&#34;color:#204a87;font-weight:bold&#34;&gt;else&lt;/span&gt;
 slurm steps &amp;lt;jobid&amp;gt;     show memory usage of running srun job steps

Show usage and fair-share values from accounting database:
 slurm h&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;&lt;span style=&#34;color:#204a87&#34;&gt;history&lt;/span&gt; &amp;lt;time&amp;gt;  show &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt; finished since, e.g. &lt;span style=&#34;color:#4e9a06&#34;&gt;&amp;#34;1day&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;default&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
 slurm shares

Show nodes and resources in the cluster:
 slurm p&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;partitions      all partitions
 slurm n&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;nodes           all cluster nodes
 slurm c&lt;span style=&#34;color:#000;font-weight:bold&#34;&gt;|&lt;/span&gt;cpus            total cpu cores in use
 slurm cpus &amp;lt;partition&amp;gt;  cores available to partition, allocated and free
 slurm cpus &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;         cores/memory reserved by running &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm cpus queue        cores/memory required by pending &lt;span style=&#34;color:#204a87&#34;&gt;jobs&lt;/span&gt;
 slurm features          List features and GRES
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;jobinfo-ltjobidgt&#34;&gt;jobinfo &amp;lt;jobid&amp;gt;&lt;/h4&gt;
&lt;p&gt;Shows status and resource consumption of jobs&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;[&lt;/span&gt;john.doe@clip-login-0 ~&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;]&lt;/span&gt;$ jobinfo &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;22&lt;/span&gt;
Name                : bash
User                : john.doe
Partition           : c
Nodes               : clip-c2-0
Cores               : &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt;
State               : COMPLETED
Submit              : 2019-07-19T11:20:14
Start               : 2019-07-19T11:20:14
End                 : 2019-07-19T11:21:37
Reserved walltime   : 08:00:00
Used walltime       : 00:01:23
Used CPU &lt;span style=&#34;color:#204a87&#34;&gt;time&lt;/span&gt;       : --
% User &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;Computation&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;:  0.00%
% System &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;I/O&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;      : 30.19%
Mem reserved        : 4G/core
Max Mem used        : 2.32M &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;clip-c2-0&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
Max Disk Write      : 10.24K &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;clip-c2-0&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
Max Disk Read       : 1.99M &lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;(&lt;/span&gt;clip-c2-0&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;cbe-slurm&#34;&gt;CBE SLURM&lt;/h2&gt;
&lt;h3 id=&#34;available-resources&#34;&gt;Available Resources&lt;/h3&gt;
&lt;p&gt;The hardware resources available to CLIP instances differ from the resources available to the physical compute nodes running the virtual&lt;/p&gt;
&lt;h3 id=&#34;compute-node-types--hardware-resources&#34;&gt;Compute Node Types &amp;amp; Hardware Resources&lt;/h3&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Node&lt;/th&gt;
&lt;th&gt;Count&lt;/th&gt;
&lt;th&gt;Cores&lt;/th&gt;
&lt;th&gt;Memory&lt;/th&gt;
&lt;th&gt;Network&lt;/th&gt;
&lt;th&gt;GPUs&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;clip-c1-x&lt;/td&gt;
&lt;td&gt;39&lt;/td&gt;
&lt;td&gt;2 x 11&lt;/td&gt;
&lt;td&gt;85 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-c2-x&lt;/td&gt;
&lt;td&gt;120&lt;/td&gt;
&lt;td&gt;2 x 19&lt;/td&gt;
&lt;td&gt;170 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-m1-x&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2 x 11&lt;/td&gt;
&lt;td&gt;4 TB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-m2-x&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;8 x 76&lt;/td&gt;
&lt;td&gt;85 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-g1-x&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2 x 7&lt;/td&gt;
&lt;td&gt;170 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;8x NVIDIA P100 (12GB VRAM)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;clip-g2-x&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;2 x 15&lt;/td&gt;
&lt;td&gt;170 GB&lt;/td&gt;
&lt;td&gt;100 Gbit/s&lt;/td&gt;
&lt;td&gt;4x NVIDIA V100 (32GB VRAM)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;node-preference&#34;&gt;Node Preference&lt;/h3&gt;
&lt;p&gt;Given that more than one node type could serve the limits as requested through jobs the nodes with the lower weight are more likely to be selected to execute the job. This implies that nodes requesting a lower amount of CPUs and memory will be preferably execute on nodes without GPUs, exrtra large memory, SSD drives, etc&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;clip-c1-x&lt;/th&gt;
&lt;th&gt;clip-c2-x&lt;/th&gt;
&lt;th&gt;clip-m1-x&lt;/th&gt;
&lt;th&gt;clip-m2-x&lt;/th&gt;
&lt;th&gt;clip-g1-x&lt;/th&gt;
&lt;th&gt;clip-g2-x&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Weight&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;partitions&#34;&gt;Partitions&lt;/h3&gt;
&lt;p&gt;On CLIP Batch Environment the nodes in the cluster are grouped in partitions. As already introduced partitions are a logical grouping of nodes of a particular quality or configuration. Within CLIP Batch Environment the following special nodes exist. The SLURM notion of partitions can be understood as queues within other resource managers like PBS, LSF, etc.&lt;/p&gt;
&lt;h4 id=&#34;rationale&#34;&gt;Rationale&lt;/h4&gt;
&lt;p&gt;SLURM partition configuration is done is such a way that jobs that could potentially run on any node type can be executed on any node of a given type. Ie all &amp;lsquo;regular&amp;rsquo; compute nodes are grouped into one partition, all &amp;lsquo;high memory&amp;rsquo; nodes are grouped in another partition and so on. Jobs that should or must be executed on a certain node type will need to explicitly either select the partition like the high memory node partition or the GPU partition.&lt;/p&gt;







&lt;div class=&#34;card rounded p-2 td-post-card mb-4 mt-4&#34; style=&#34;max-width: 823px&#34;&gt;
	&lt;img class=&#34;card-img-top&#34; src=&#34;https://clip-hpc.github.io/docs/cbe/slurm/cbe-nodes_hube6f268fa83de819f54b31abfd809e15_293759_813x0_resize_catmullrom_2.png&#34; width=&#34;813&#34; height=&#34;567&#34;&gt;
	
	&lt;div class=&#34;card-body px-0 pt-2 pb-0&#34;&gt;
		&lt;p class=&#34;card-text&#34;&gt;
CLIP Nodes
&lt;/p&gt;
	&lt;/div&gt;
	
&lt;/div&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Partition:&lt;/th&gt;
&lt;th&gt;c&lt;/th&gt;
&lt;th&gt;m&lt;/th&gt;
&lt;th&gt;g&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Use Case&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;default partition&lt;/strong&gt; where jobs can be submitted to&lt;/td&gt;
&lt;td&gt;special use partition for jobs that require from these nodes with are significantly enlarged main memory/RAM within compute nodes&lt;/td&gt;
&lt;td&gt;special use partition for jobs that profit from GPU accelerators within GPU nodes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Resources&lt;/td&gt;
&lt;td&gt;Contains all regular high core count (clip-c2-x) and high clock (clip-c1-x) compute nodes&lt;/td&gt;
&lt;td&gt;Contains only nodes of the large  Memory Node type for large memory (clip-m2-x) and very large memory (clip-m1-x) nodes&lt;/td&gt;
&lt;td&gt;Contains only nodes of the GPU Node type with both GPU types (clip-g1-x &amp;amp; clip-g2-x)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;resource-limits-qos&#34;&gt;Resource Limits (QOS)&lt;/h3&gt;
&lt;p&gt;Within SLURM resource limitations are configured through named objects called QOS (Quality of Service). QOS objects specify limits in on resource like CPUs, memory, etc , and maximum execution time (wall time). QOS objects be attached to partitions and users may specify QOS objects on job submission (srun, salloc, sbatch).&lt;/p&gt;
&lt;p&gt;The CBE resource isolation is an configured to use Linux cgroups a feature that setups a resource &amp;ldquo;jail&amp;rdquo; enforced by the Kernel to guarantee resource and reduce job interference between jobs sharing and executing on the same compute node to a minimum.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;QOS:&lt;/th&gt;
&lt;th&gt;short&lt;/th&gt;
&lt;th&gt;medium&lt;/th&gt;
&lt;th&gt;long&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Use Case&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Default&lt;/strong&gt; QOS (ie when no QOS is selected then this QOS is assumed)&lt;br&gt;Short running jobs with large resource limits (fast turnaround)&lt;/td&gt;
&lt;td&gt;&amp;ldquo;Average&amp;rdquo; jobs&lt;/td&gt;
&lt;td&gt;Very long running jobs with small resource limits&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Limits&lt;/td&gt;
&lt;td&gt;Dynamic (check out put of &amp;lsquo;slurm qos&amp;rsquo;)&lt;/td&gt;
&lt;td&gt;Dynamic (check out put of &amp;lsquo;slurm qos&amp;rsquo;)&lt;/td&gt;
&lt;td&gt;Dynamic (check out put of &amp;lsquo;slurm qos&amp;rsquo;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Walltime&lt;/td&gt;
&lt;td&gt;8 hours&lt;br&gt;(08:00:00)&lt;/td&gt;
&lt;td&gt;2 days&lt;br&gt;(2-00:00:00)&lt;/td&gt;
&lt;td&gt;14 days&lt;br&gt;(14-00:00:00)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;scheduling-policy&#34;&gt;Scheduling Policy&lt;/h3&gt;
&lt;h4 id=&#34;backfill-strategy&#34;&gt;Backfill Strategy&lt;/h4&gt;
&lt;p&gt;The order of job admission onto compute nodes is determined through the configured scheduling policy. CBE SURM is configured for the &amp;ldquo;backfill&amp;rdquo; strategy.  Without backfill scheduling, each partition is scheduled strictly in priority order, which typically results in significantly lower system utilization and responsiveness than otherwise possible. Backfill scheduling will start lower priority jobs if doing so does not delay the expected start time of any higher priority jobs. Since the expected start time of pending jobs depends upon the expected completion time of running jobs, reasonably accurate time limits are important for backfill scheduling to work well.&lt;/p&gt;
&lt;p&gt;Slurm&amp;rsquo;s backfill scheduler takes into consideration every running job. It then considers pending jobs in priority order, determining when and where each will start, taking into consideration the possibility of resource requirements, etc. If the job under consideration can start immediately without impacting the expected start time of any higher priority job, then it does so. Otherwise the resources required by the job will be reserved during the job&amp;rsquo;s expected execution time.&lt;/p&gt;
&lt;p&gt;The backfill plugin will set the expected start time for pending jobs. A job&amp;rsquo;s expected start time can be seen using the &lt;code&gt;squeue --start command&lt;/code&gt;.&lt;/p&gt;
&lt;h4 id=&#34;prioritization&#34;&gt;Prioritization&lt;/h4&gt;
&lt;p&gt;The prioritisation configuration uses the multifactor prioritisation configuration and the following factors are taken into consideration to compute the sequence in which job are admitted onto partitions/computing resources and how pending jobs are sorted in the queue.&lt;/p&gt;
&lt;h5 id=&#34;factors&#34;&gt;Factors&lt;/h5&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Factor&lt;/th&gt;
&lt;th&gt;What is it&lt;/th&gt;
&lt;th&gt;Explanation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Age&lt;/td&gt;
&lt;td&gt;the length of time a job has been waiting in the queue, eligible to be scheduled&lt;/td&gt;
&lt;td&gt;In general, the longer a job waits in the queue, the larger its age factor grows. However, the age factor for a dependent job will not change while it waits for the job it depends on to complete. Also, the age factor will not change when scheduling is withheld for a job whose node or time limits exceed the cluster&amp;rsquo;s current limits.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fair-Share (Usage)&lt;/td&gt;
&lt;td&gt;the fair-share factor serves to prioritize queued jobs such that those jobs charging accounts that are under-serviced are scheduled first, while jobs charging accounts that are over-serviced are scheduled when the machine would otherwise go idle.&lt;/td&gt;
&lt;td&gt;All jobs submitted at the time of this writing submit to the same account name (&amp;ldquo;root&amp;rdquo;) - this will change as soon as the cloud partition is in full configuration SLURM&amp;rsquo;s fair-share factor is a floating point number between 0.0 and 1.0 that reflects the shares of a computing resource that a user has been allocated and the amount of computing resources the user&amp;rsquo;s jobs have consumed. The higher the value, the higher is the placement in the queue of jobs waiting to be scheduled.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Partition&lt;/td&gt;
&lt;td&gt;a factor associated with each partition - see the partition section of this document&lt;/td&gt;
&lt;td&gt;it will favor partitions that have the bulk of the nodes&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QOS&lt;/td&gt;
&lt;td&gt;a factor associated with each Quality Of Service - see the QOS section for the priority&lt;/td&gt;
&lt;td&gt;it will favor short running jobs that are submitted to the short qos&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Jos Size (TRES)&lt;/td&gt;
&lt;td&gt;Size of the job based on Trackable Resources (TRES) such as Core, Memory, etc&lt;/td&gt;
&lt;td&gt;Within the same partition and qos favor small jobs&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h5 id=&#34;formula&#34;&gt;Formula&lt;/h5&gt;
&lt;p&gt;All of the factors in this formula are:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Job_priority =
    (PriorityWeightAge) * (age_factor) +
    (PriorityWeightFairshare) * (fair-share_factor) +
    (PriorityWeightJobSize) * (job_size_factor) +
    (PriorityWeightPartition) * (partition_factor) +
    (PriorityWeightQOS) * (QOS_factor) +
    SUM(TRES_weight_cpu * TRES_factor_cpu,
        TRES_weight_&amp;lt;type&amp;gt; * TRES_factor_&amp;lt;type&amp;gt;,
     ...)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The usage (fair share) can be retrieved by running the &lt;code&gt;slurm&lt;/code&gt; command with the option &lt;code&gt;shares&lt;/code&gt;
The priority of individual jobs can be trieved using the &lt;code&gt;sprio&lt;/code&gt; command for waiting jobs.&lt;/p&gt;

      </description>
    </item>
    
    <item>
      <title>Docs: Interactive workloads</title>
      <link>https://clip-hpc.github.io/docs/cbe/interactive/</link>
      <pubDate>Thu, 05 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://clip-hpc.github.io/docs/cbe/interactive/</guid>
      <description>
        
        
        &lt;p&gt;To ensure short start times for interactive workloads, we have created a recurring reservation for such jobs.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ReservationName=interactive StartTime=2020-03-18T09:00:00 EndTime=2020-03-18T17:00:00 Duration=08:00:00
   Nodes=clip-c1-[9-11],clip-c2-0 NodeCnt=4 CoreCnt=88 Features=(null) PartitionName=c Flags=FLEX,WEEKDAY,REPLACE,NO_HOLD_JOBS_AFTER_END
   TRES=cpu=88
   Users=(null) Accounts=gmi,imp,imba,vbcf,hephy,smi Licenses=(null) State=ACTIVE BurstBuffer=(null) Watts=n/a
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The &lt;strong&gt;name of the reservation&lt;/strong&gt; is “&lt;strong&gt;interactive&lt;/strong&gt;“. Jobs from interactive services (RStudio, XPRA, Jupyterhub) will launch on this reservation, but also sbatch / srun submitted jobs can be sent to it.&lt;/p&gt;


&lt;div class=&#34;alert alert-info&#34; role=&#34;alert&#34;&gt;
&lt;h4 class=&#34;alert-heading&#34;&gt;Important notes&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;the reservation is ACTIVE from 0900 to 1700, repeating every WEEKDAY.&lt;/li&gt;
&lt;li&gt;the reservation only contains nodes from &amp;ldquo;c&amp;rdquo; partition&lt;/li&gt;
&lt;li&gt;jobs started within the reservation will continue running once the reservation becomes inactive&lt;/li&gt;
&lt;li&gt;if enough idle resources are available jobs submitted to the reservation can also run on nodes not part of the reservation&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;h2 id=&#34;how-to-submit-interactive-jobs&#34;&gt;How to submit interactive jobs&lt;/h2&gt;
&lt;p&gt;Keep your jobs as small as necessary (cpus, memory). Use &amp;ndash;time to set appropriate time limits for your job. Use this resource responsibly!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#8f5902;font-style:italic&#34;&gt;# example interactive job&lt;/span&gt;
srun -n &lt;span style=&#34;color:#0000cf;font-weight:bold&#34;&gt;1&lt;/span&gt; --mem&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1g --time&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;1:00:00 --reservation&lt;span style=&#34;color:#ce5c00;font-weight:bold&#34;&gt;=&lt;/span&gt;interactive --pty bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will launch a job with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 task (core), 1 GB memory, that will expire after 1 hour.&lt;/li&gt;
&lt;li&gt;the resources will be taken from the reservation &amp;ldquo;&lt;code&gt;interactive&lt;/code&gt;&amp;rdquo;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;--pty&lt;/code&gt; attaches the terminal to the first process in the job (the bash shell), i.e. you prompt will continue inside the allocated job&lt;/li&gt;
&lt;/ul&gt;

      </description>
    </item>
    
  </channel>
</rss>
